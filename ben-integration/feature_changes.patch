diff --git a/.gitignore b/.gitignore
index b39675e..55c8fc8 100644
--- a/.gitignore
+++ b/.gitignore
@@ -44,10 +44,7 @@ htmlcov/
 
 # Logs
 *.log
-logs/
-
-# Stream output
-stream_output.json
+/logs
 
 # OS
 .DS_Store
@@ -58,10 +55,4 @@ Thumbs.db
 *.mp3
 *.m4a
 
-# Frontend
-frontend/node_modules/
-frontend/dist/
-frontend/build/
-frontend/.next/
-frontend/out/
-frontend/.nuxt/
+
diff --git a/docs/guides/AUDIO_TESTING_GUIDE.md b/AUDIO_TESTING_GUIDE.md
similarity index 100%
rename from docs/guides/AUDIO_TESTING_GUIDE.md
rename to AUDIO_TESTING_GUIDE.md
diff --git a/BATCHED_SELECTION.md b/BATCHED_SELECTION.md
new file mode 100644
index 0000000..ce1ad2c
--- /dev/null
+++ b/BATCHED_SELECTION.md
@@ -0,0 +1,127 @@
+# Batched Claim Selection - Implementation Summary
+
+## What Changed
+
+Replaced **per-sentence fact-checking** with **batched intelligent selection**.
+
+### Before (Old Approach)
+```
+Every final sentence â†’ Queue for fact-check â†’ Process one by one
+```
+- âŒ Wastes API calls on greetings, opinions, filler
+- âŒ Queue fills with irrelevant sentences  
+- âŒ Important claims delayed behind junk
+
+### After (New Approach)
+```
+Accumulate 10 sentences â†’ LLM selects top 2 claims â†’ Queue only those
+```
+- âœ… One LLM call processes 10 sentences
+- âœ… Prioritizes important/controversial claims
+- âœ… Filters out opinions, greetings, filler
+- âœ… More efficient use of API quota
+
+## Configuration
+
+In `config.py`:
+```python
+claim_selection_batch_size: int = 10  # accumulate N sentences
+max_claims_per_batch: int = 2         # select top N claims
+```
+
+Tune these based on:
+- **Smaller batch** (5): More responsive, catches claims faster
+- **Larger batch** (15): More context for better selection
+- **More claims** (3): Less filtering, catches more
+- **Fewer claims** (1): Very selective, only most important
+
+## How It Works
+
+### 1. Accumulation (state_manager.py)
+```python
+state.sentence_batch = []  # Stores recent final sentences
+```
+
+### 2. Selection (fact_engine.py)
+When batch reaches size 10:
+```python
+async def select_claims(statements: List[str]) -> List[str]:
+    # Single LLM call analyzes all statements
+    # Returns top 2 most important factual claims
+```
+
+### 3. Queueing (main.py)
+```python
+selected_claims = await fact_engine.select_claims(batch)
+for claim in selected_claims:
+    await state.fact_queue.put(claim)  # Only queue selected ones
+```
+
+### 4. Processing (fact_engine.py)
+Background worker processes selected claims through 3-step pipeline:
+- Step 1: ~~Detect claim~~ (SKIPPED - already selected)
+- Step 2: Search evidence
+- Step 3: Verify claim
+
+## Selection Criteria
+
+The LLM selects claims that are:
+- âœ… Specific and concrete (not vague)
+- âœ… Verifiable (numbers, dates, events, facts)
+- âœ… Potentially controversial or surprising
+- âœ… About objective reality
+
+Automatically filters out:
+- âŒ Greetings ("Hello everyone")
+- âŒ Opinions ("I think that's cool")
+- âŒ Questions ("What do you mean?")
+- âŒ Filler ("Um, let me see...")
+
+## Files Modified
+
+1. **config.py**
+   - Added batch size and max claims settings
+   - Added CLAIM_SELECTION_PROMPT import
+
+2. **CLAIM_SELECTION_PROMPT.txt** (NEW)
+   - Prompt for intelligent claim selection
+
+3. **state_manager.py**
+   - Added `sentence_batch` list
+
+4. **fact_engine.py**
+   - Added `select_claims()` method
+   - Import CLAIM_SELECTION_PROMPT
+
+5. **main.py**
+   - Replaced `queue_fact_check_async()` call
+   - Added `batch_claim_selection_async()` function
+   - Old function kept for reference
+
+## Testing
+
+Run a test and observe the difference:
+```bash
+python main.py
+# In another terminal:
+python tests/test_simple_stream.py
+```
+
+You should see:
+- Fewer claims being queued
+- Only important/factual statements selected
+- `claim_selected` events instead of `fact_queued`
+
+## Next Steps for Tuning
+
+1. **Adjust batch size** if claims arrive too slowly/quickly
+2. **Adjust max claims** if too strict/lenient  
+3. **Modify selection prompt** to change what counts as "important"
+4. **Monitor via logs** which claims get selected vs ignored
+
+## Expected Benefits
+
+- ğŸš€ **5-10x reduction** in fact-check volume
+- ğŸ’° **Lower API costs** (fewer LLM + search calls)
+- âš¡ **Faster processing** (queue doesn't back up)
+- ğŸ¯ **Better quality** (focus on what matters)
diff --git a/CLAIM_SELECTION_PROMPT.txt b/CLAIM_SELECTION_PROMPT.txt
new file mode 100644
index 0000000..7f06c36
--- /dev/null
+++ b/CLAIM_SELECTION_PROMPT.txt
@@ -0,0 +1,33 @@
+You are analyzing a conversation transcript to identify important factual claims worth fact-checking.
+
+Here is the recent conversation:
+
+{text}
+
+Your task:
+1. Identify VERIFIABLE FACTUAL CLAIMS in this conversation
+2. Prioritize claims that are:
+   - Specific and concrete (numbers, dates, events, names)
+   - Potentially controversial or surprising
+   - About objective facts that can be verified
+3. Extract up to {max_claims} complete claims with enough context
+
+DO NOT select:
+- Pure opinions ("I think...", "I feel...")
+- Vague statements without specifics
+- Greetings or filler words
+- Questions
+
+Important: Extract the FULL CLAIM with context, not just fragments.
+
+Respond in JSON format:
+{{
+    "selected_claims": [
+        {{
+            "claim": "the complete factual claim with sufficient context",
+            "reason": "why this is important to verify"
+        }}
+    ]
+}}
+
+If no important factual claims exist, return: {{"selected_claims": []}}
\ No newline at end of file
diff --git a/docs/CLAUDE.md b/CLAUDE.md
similarity index 100%
rename from docs/CLAUDE.md
rename to CLAUDE.md
diff --git a/docs/guides/DEEPGRAM_V2_UPDATE.md b/DEEPGRAM_V2_UPDATE.md
similarity index 100%
rename from docs/guides/DEEPGRAM_V2_UPDATE.md
rename to DEEPGRAM_V2_UPDATE.md
diff --git a/docs/guides/DEMO_COMMANDS.md b/DEMO_COMMANDS.md
similarity index 100%
rename from docs/guides/DEMO_COMMANDS.md
rename to DEMO_COMMANDS.md
diff --git a/docs/guides/DEMO_SETUP.md b/DEMO_SETUP.md
similarity index 100%
rename from docs/guides/DEMO_SETUP.md
rename to DEMO_SETUP.md
diff --git a/FFMPEG_INSTALLATION.md b/FFMPEG_INSTALLATION.md
new file mode 100644
index 0000000..7ded335
--- /dev/null
+++ b/FFMPEG_INSTALLATION.md
@@ -0,0 +1,174 @@
+# FFmpeg Installation Guide
+
+FFmpeg is required to convert audio to the format Deepgram expects.
+
+## Quick Install (Windows)
+
+### Option 1: Download Pre-built Binaries (Easiest)
+
+1. **Download FFmpeg:**
+   - Visit: https://www.gyan.dev/ffmpeg/builds/
+   - Download: **ffmpeg-release-essentials.zip** (smallest, recommended)
+   - Or download: **ffmpeg-release-full.zip** (includes extra features)
+
+2. **Extract:**
+   ```
+   Extract the ZIP to: C:\ffmpeg
+   ```
+
+3. **Verify structure:**
+   ```
+   C:\ffmpeg\
+     â””â”€â”€ bin\
+         â”œâ”€â”€ ffmpeg.exe   â† This is what we need
+         â”œâ”€â”€ ffplay.exe
+         â””â”€â”€ ffprobe.exe
+   ```
+
+4. **Test it:**
+   ```bash
+   C:\ffmpeg\bin\ffmpeg.exe -version
+   ```
+
+   Should output FFmpeg version info.
+
+5. **Update the script:**
+   The script is already configured to use `C:\ffmpeg\bin\ffmpeg.exe`
+
+   If you extracted to a different location, edit `test_audio_client.py` line 16:
+   ```python
+   FFMPEG_PATH = r"C:\your\path\to\ffmpeg.exe"
+   ```
+
+### Option 2: Install with Chocolatey
+
+If you have Chocolatey installed:
+
+```bash
+choco install ffmpeg
+```
+
+Then update `test_audio_client.py`:
+```python
+FFMPEG_PATH = "ffmpeg"  # Will use system PATH
+```
+
+### Option 3: Add to System PATH (Optional)
+
+After downloading, you can add FFmpeg to your system PATH:
+
+1. Press `Win + X` â†’ System
+2. Advanced system settings â†’ Environment Variables
+3. Under System Variables, find `Path`
+4. Click Edit â†’ New
+5. Add: `C:\ffmpeg\bin`
+6. Click OK on all dialogs
+7. Restart terminal
+
+Then update `test_audio_client.py`:
+```python
+FFMPEG_PATH = "ffmpeg"  # Will use system PATH
+```
+
+## Verification
+
+Run this to verify FFmpeg is working:
+
+```bash
+# If installed to C:\ffmpeg
+C:\ffmpeg\bin\ffmpeg.exe -version
+
+# If added to PATH
+ffmpeg -version
+```
+
+Expected output:
+```
+ffmpeg version N-xxxxx-xxx
+built with gcc x.x.x
+configuration: ...
+```
+
+## What the Script Does with FFmpeg
+
+The script uses FFmpeg to convert your audio files to the format Deepgram requires:
+
+```
+Input: Your podcast.mp3 (or .wav, .m4a, etc.)
+   â†“
+FFmpeg: Converts to linear16 PCM, 16kHz, mono
+   â†“
+Output: Raw audio bytes
+   â†“
+Server: Sends to Deepgram for transcription
+```
+
+**Command used internally:**
+```bash
+ffmpeg -i input.mp3 \
+  -f s16le \        # 16-bit little-endian PCM
+  -ar 16000 \       # 16kHz sample rate
+  -ac 1 \           # Mono (1 channel)
+  -                 # Output to stdout
+```
+
+## Troubleshooting
+
+### "ffmpeg is not recognized as an internal or external command"
+
+**Cause:** FFmpeg not in PATH and FFMPEG_PATH is set to just `"ffmpeg"`
+
+**Fix:**
+- Option A: Use full path in `test_audio_client.py`
+- Option B: Add FFmpeg to system PATH (see Option 3 above)
+
+### "Permission denied" or "Access denied"
+
+**Cause:** Windows security blocking execution
+
+**Fix:**
+1. Right-click ffmpeg.exe â†’ Properties
+2. Check "Unblock" if present â†’ Apply
+3. Or run terminal as Administrator
+
+### FFmpeg downloaded but script still can't find it
+
+**Check:**
+1. Verify file is at: `C:\ffmpeg\bin\ffmpeg.exe`
+2. Check `test_audio_client.py` line 16 matches your path
+3. Use absolute path (not relative)
+
+### FFmpeg works but audio conversion fails
+
+**Cause:** Corrupted audio file or unsupported format
+
+**Fix:**
+1. Try a different audio file
+2. Ensure file isn't empty
+3. Check FFmpeg supports the format:
+   ```bash
+   ffmpeg -formats
+   ```
+
+## Alternative Download Sources
+
+If the above link doesn't work:
+
+- **Official FFmpeg builds**: https://ffmpeg.org/download.html#build-windows
+- **GitHub releases**: https://github.com/BtbN/FFmpeg-Builds/releases
+- **Chocolatey package**: https://community.chocolatey.org/packages/ffmpeg
+
+## After Installation
+
+Once FFmpeg is installed, you can run:
+
+```bash
+python test_audio_client.py
+```
+
+You should see:
+```
+âœ“ FFmpeg found
+```
+
+Instead of the error message.
diff --git a/FIXES_APPLIED.md b/FIXES_APPLIED.md
new file mode 100644
index 0000000..dabad71
--- /dev/null
+++ b/FIXES_APPLIED.md
@@ -0,0 +1,220 @@
+# Fixes Applied - Deepgram Connection Issues Resolved
+
+## Issue 1: Async Context Manager Error âœ… FIXED
+
+**Error:**
+```
+object _AsyncGeneratorContextManager can't be used in 'await' expression
+```
+
+**Root Cause:**
+`deepgram.listen.v2.connect()` returns an async context manager that must be used with `async with`, not `await`.
+
+**Fix Applied:**
+```python
+# Before (WRONG)
+dg_connection = await deepgram.listen.v2.connect(...)
+
+# After (CORRECT)
+async with deepgram.listen.v2.connect(...) as dg_connection:
+    # All Deepgram code here
+```
+
+**Files Changed:**
+- `main.py:137-247` - Wrapped connection in async context manager
+
+## Issue 2: DuckDuckGo Search Deprecation Warning âœ… FIXED
+
+**Warning:**
+```
+RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`!
+```
+
+**Fix Applied:**
+1. Updated `requirements.txt` to use `ddgs==1.0.0`
+2. Added fallback import in `fact_engine.py`:
+```python
+try:
+    from ddgs import DDGS
+except ImportError:
+    from duckduckgo_search import DDGS  # Fallback
+```
+
+**Files Changed:**
+- `requirements.txt:13` - Changed package name
+- `fact_engine.py:11-14` - Added fallback import
+
+## Issue 3: Indentation Errors âœ… FIXED
+
+**Problem:**
+Event handler functions had incorrect indentation after adding async context manager.
+
+**Fix Applied:**
+- Properly indented all code inside the `async with` block
+- Event handlers (`on_message`, `on_error`, `on_open`, `on_close`) now correctly indented
+- Main loop and cleanup code properly nested
+
+**Files Changed:**
+- `main.py:144-247` - Fixed all indentation
+
+## Server Status
+
+âœ… **Server is now running correctly**
+
+The server should start without errors:
+```bash
+python main.py
+```
+
+## Test Client
+
+The test client should now connect successfully:
+```bash
+python test_client.py
+```
+
+## Expected Output
+
+You should see:
+```
+INFO:     Started server process
+INFO:     Waiting for application startup.
+INFO:     Application startup complete.
+```
+
+And when test client connects:
+```
+WebSocket connection accepted from Address(host='127.0.0.1', port=...)
+Deepgram connection opened
+```
+
+## Remaining Warnings (Non-Critical)
+
+These warnings can be ignored for now:
+
+1. **Pydantic Deprecation Warning:**
+   - Coming from Deepgram SDK's internal use of Pydantic v1 config
+   - Will be fixed in future Deepgram SDK update
+   - Does not affect functionality
+
+2. **WebSockets Deprecation Warning:**
+   - Coming from uvicorn's use of older websockets API
+   - Will be fixed in future uvicorn update
+   - Does not affect functionality
+
+## Issue 4: Deepgram HTTP 400 Error âœ… FIXED
+
+**Error:**
+```
+Failed to initialize Deepgram: status_code: 400
+server rejected WebSocket connection: HTTP 400
+body: Unexpected error when initializing websocket connection.
+```
+
+**Root Cause:**
+The Deepgram connection parameters didn't match the WAV file audio format:
+- WAV file: 48kHz, 16-bit, stereo (2 channels)
+- Original code: 16kHz sample rate âŒ MISMATCH
+
+**Fix Applied:**
+```python
+# Before (WRONG)
+async with deepgram.listen.v2.connect(
+    model="nova-2",
+    encoding="linear16",
+    sample_rate="16000"  # âŒ Doesn't match WAV file
+) as dg_connection:
+
+# After (CORRECT)
+options = {
+    "model": "nova-2",
+    "encoding": "linear16",
+    "sample_rate": 48000,  # âœ… Matches WAV file
+    "channels": 2,  # âœ… Stereo
+    "interim_results": True,
+    "smart_format": True,
+}
+async with deepgram.listen.v2.connect(**options) as dg_connection:
+```
+
+**Files Changed:**
+- `main.py:136-148` - Updated Deepgram connection parameters
+
+## Issue 5: WebSocket Control Frame Too Long âœ… FIXED
+
+**Error:**
+```
+websockets.exceptions.ProtocolError: control frame too long
+```
+
+**Root Cause:**
+WebSocket CLOSE frames have a maximum payload of 125 bytes. The error message being sent was too long (over 200 chars).
+
+**Fix Applied:**
+```python
+# Before (WRONG)
+except Exception as e:
+    logger.error(f"Failed to initialize Deepgram: {e}")
+    await websocket.close(code=1011, reason=str(e))  # âŒ Too long
+
+# After (CORRECT)
+except Exception as e:
+    logger.error(f"Failed to initialize Deepgram: {e}")
+    error_msg = str(e)[:100]  # âœ… Truncated to 100 chars
+    await websocket.close(code=1011, reason=error_msg)
+```
+
+**Files Changed:**
+- `main.py:253-255` - Truncate error messages to 100 characters
+
+## Testing Instructions
+
+1. **Start the server:**
+   ```bash
+   python main.py
+   ```
+
+2. **In another terminal, stream the WAV file:**
+   ```bash
+   python test_wav_stream.py
+   ```
+
+## Expected Output
+
+### Server Console (main.py):
+```
+INFO:     Uvicorn running on http://0.0.0.0:8000
+2025-11-22 - INFO - Starting Real-Time Podcast AI Assistant
+INFO:     ('127.0.0.1', xxxxx) - "WebSocket /listen" [accepted]
+2025-11-22 - INFO - WebSocket connection accepted
+2025-11-22 - INFO - Deepgram connection opened  â† âœ… SUCCESS!
+2025-11-22 - INFO - [FINAL] Welcome to today's podcast...
+```
+
+### Client Console (test_wav_stream.py):
+```
+============================================================
+Real-Time Podcast AI Assistant - WAV Stream Test
+============================================================
+âœ“ Connected to server
+ğŸ™ï¸  Starting audio stream...
+============================================================
+
+ğŸ¤ FINAL [0.95] Welcome to today's podcast...  â† âœ… TRANSCRIPTIONS!
+â¸ï¸  PARTIAL [0.82] We're discussing...
+ğŸ“Š TOPIC UPDATE: Artificial Intelligence
+```
+
+## If Issues Persist
+
+### Still Getting HTTP 400?
+1. Check API key in `.env` is valid
+2. Verify Deepgram account has credits
+3. Check WAV file format matches server config
+
+### No Transcriptions?
+1. Ensure WAV file has actual speech (not silence)
+2. Look at server logs for errors
+3. Test with `python test_wav_stream.py --test` first
+
+All critical issues are now resolved! ğŸ‰
diff --git a/docs/guides/MIGRATION_NOTES.md b/MIGRATION_NOTES.md
similarity index 100%
rename from docs/guides/MIGRATION_NOTES.md
rename to MIGRATION_NOTES.md
diff --git a/docs/guides/QUICKSTART.md b/QUICKSTART.md
similarity index 100%
rename from docs/guides/QUICKSTART.md
rename to QUICKSTART.md
diff --git a/README.md b/README.md
index 799d561..c6cca9e 100644
--- a/README.md
+++ b/README.md
@@ -1,84 +1,216 @@
 # Real-Time Podcast AI Assistant
 
-A real-time AI assistant for podcast transcription, topic tracking, and fact-checking.
-
-## Project Structure
-
-```
-lauzhack-2025/
-â”œâ”€â”€ backend/             # Backend FastAPI application
-â”‚   â”œâ”€â”€ app/
-â”‚   â”‚   â”œâ”€â”€ api/        # API endpoints and WebSocket handlers
-â”‚   â”‚   â”œâ”€â”€ core/       # Core configuration and state management
-â”‚   â”‚   â”œâ”€â”€ engines/    # Topic and fact-checking engines
-â”‚   â”‚   â””â”€â”€ utils/      # Utility functions
-â”‚   â”œâ”€â”€ tests/          # Test files and test data
-â”‚   â”œâ”€â”€ run.py          # Backend entry point
-â”‚   â””â”€â”€ requirements.txt # Python dependencies
-â”œâ”€â”€ frontend/           # Frontend application (to be implemented)
-â”œâ”€â”€ docs/              # Documentation and guides
-â””â”€â”€ README.md          # This file
-```
+An AI-powered assistant for live podcast transcription, topic tracking, and real-time fact-checking.
 
 ## Features
 
-- **Real-time Transcription**: Using Deepgram's WebSocket API for live audio transcription
-- **Topic Tracking**: Semantic drift detection and topic tree generation
-- **Fact Checking**: 3-step pipeline (Detect â†’ Search â†’ Verify) for verifying claims
-- **WebSocket API**: Real-time bidirectional communication
+- **Live Transcription**: Real-time audio transcription using Deepgram Flux
+- **Topic Tracking**: Automatic semantic drift detection and conversation timeline
+- **Fact Checking**: 3-step verification pipeline (Detect â†’ Search â†’ Verify)
+- **Dual-Loop Architecture**: Fast loop for topics, slow loop for fact-checking
+- **WebSocket API**: Real-time communication with minimal latency
 
-## Getting Started
+## Architecture
 
-### Backend Setup
+```
+â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
+â”‚                     Client (Audio Stream)                    â”‚
+â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
+                           â”‚ WebSocket
+                           â–¼
+â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
+â”‚                       FastAPI Server                         â”‚
+â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
+â”‚  â”‚              Deepgram (Transcription)                   â”‚ â”‚
+â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
+â”‚                 â”‚                                             â”‚
+â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
+â”‚     â”‚   FAST LOOP          â”‚       â”‚   SLOW LOOP         â”‚   â”‚
+â”‚     â”‚   Topic Tracker      â”‚       â”‚   Fact Checker      â”‚   â”‚
+â”‚     â”‚   (NetworkX Graph)   â”‚       â”‚   (3-Step Pipeline) â”‚   â”‚
+â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
+â”‚                                                               â”‚
+â”‚              State Manager (Central State)                   â”‚
+â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
+```
+
+## Installation
 
-1. Navigate to the backend directory:
+1. **Clone the repository**:
    ```bash
-   cd backend
+   cd lauzhack-2025
    ```
 
-2. Install dependencies:
+2. **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```
 
-3. Configure environment variables:
+3. **Set up environment variables**:
    ```bash
    cp .env.example .env
-   # Edit .env and add your API keys
    ```
 
-4. Run the backend server:
+   Edit `.env` and add your API keys:
+   ```
+   DEEPGRAM_API_KEY=your_deepgram_key
+   TOGETHER_API_KEY=your_together_ai_key
+   HUGGINGFACE_API_KEY=your_hf_key  # Optional
+   ```
+
+4. **Verify setup** (optional but recommended):
    ```bash
-   python run.py
-   # Or: python -m backend
+   python verify_setup.py
    ```
 
-The server will start on `http://localhost:8765`
+## Usage
+
+### Start the Server
+
+```bash
+python main.py
+```
+
+The server will start on `http://localhost:8000`
+
+### API Endpoints
+
+- `GET /` - Health check
+- `GET /stats` - System statistics
+- `GET /topics` - Topic timeline
+- `GET /facts` - Recent fact-check results
+- `WebSocket /listen` - Main audio streaming endpoint
+- `WebSocket /facts/stream` - Real-time fact results stream
+
+### WebSocket Client Example
+
+```python
+import asyncio
+import websockets
+import pyaudio
+
+async def stream_audio():
+    uri = "ws://localhost:8000/listen"
+
+    async with websockets.connect(uri) as websocket:
+        # Configure audio
+        audio = pyaudio.PyAudio()
+        stream = audio.open(
+            format=pyaudio.paInt16,
+            channels=1,
+            rate=16000,
+            input=True,
+            frames_per_buffer=8000
+        )
+
+        # Stream audio
+        while True:
+            data = stream.read(8000)
+            await websocket.send(data)
+
+            # Receive transcription
+            response = await websocket.recv()
+            print(response)
 
-### Frontend Setup
+asyncio.run(stream_audio())
+```
+
+## Configuration
+
+Edit `config.py` or set environment variables:
+
+- `FACT_CHECK_RATE_LIMIT`: Seconds between fact checks (default: 10)
+- `TOPIC_UPDATE_THRESHOLD`: Sentences before topic update (default: 5)
+- `TOGETHER_MODEL`: LLM model to use (default: Llama-3.1-70B)
+
+## Project Structure
+
+```
+lauzhack-2025/
+â”œâ”€â”€ main.py              # FastAPI server & WebSocket endpoints
+â”œâ”€â”€ config.py            # Configuration & prompts
+â”œâ”€â”€ state_manager.py     # Central state management
+â”œâ”€â”€ topic_engine.py      # Topic tracking & semantic drift
+â”œâ”€â”€ fact_engine.py       # 3-step fact-checking pipeline
+â”œâ”€â”€ requirements.txt     # Python dependencies
+â”œâ”€â”€ .env.example         # Environment template
+â””â”€â”€ README.md           # This file
+```
+
+## How It Works
+
+### Fast Loop (Topic Tracking)
+
+1. Partial transcripts arrive continuously
+2. Every 5 finalized sentences â†’ extract topic using LLM
+3. Compare with current topic using embeddings
+4. If semantic drift detected â†’ create new topic node
+5. Update NetworkX graph with topic timeline
 
-The frontend is not yet implemented. See `frontend/README.md` for planned structure.
+### Slow Loop (Fact Checking)
 
-## Testing
+1. **Detect**: LLM determines if sentence contains factual claim
+2. **Search**: DuckDuckGo finds evidence (with source links)
+3. **Verify**: LLM compares claim vs evidence
+4. Rate-limited to 1 check per 10 seconds (configurable)
+5. Results stored and streamed to clients
+
+## Development
+
+### Testing
 
-Run the WAV file streaming test:
 ```bash
-cd backend/tests
-python test_wav_stream.py
+# Run the server in development mode
+python main.py
+
+# In another terminal, test the API
+curl http://localhost:8000/stats
 ```
 
-## Documentation
+### TODO Improvements
+
+- [ ] Replace mock embeddings with sentence-transformers
+- [ ] Add speaker diarization
+- [ ] Implement conversation export
+- [ ] Add authentication
+- [ ] Create web UI
+- [ ] Add database persistence
+- [ ] Implement caching for LLM responses
+
+## Tech Stack
 
-See the `docs/` directory for detailed guides:
-- Architecture and design decisions
-- API documentation
-- Deployment guides
-- Feature-specific guides
+- **Framework**: FastAPI 0.115.0
+- **Audio**: Deepgram SDK 5.3.0
+- **LLM**: Together.ai 1.3.4 (Llama-3.1-70B)
+- **Search**: DuckDuckGo Search 8.1.1
+- **Graph**: NetworkX 3.4.2
+- **Async**: Python asyncio
+- **Python**: 3.8+ required
 
 ## License
 
-See [LICENSE](LICENSE) file for details.
+MIT License - Built for LauzHack 2025
 
-## Development
+## Troubleshooting
+
+**Issue**: Deepgram connection fails
+- Check API key in `.env`
+- Ensure audio format is PCM 16kHz mono
+
+**Issue**: Rate limiting on search
+- Increase `FACT_CHECK_RATE_LIMIT` in `.env`
+- Reduce `SEARCH_CONFIG.max_results` in `config.py`
+
+**Issue**: LLM responses fail to parse
+- Check Together.ai API key
+- Verify model availability
+- Check logs for JSON parsing errors
+
+## Contributing
+
+This is a hackathon MVP. Contributions welcome!
 
-This project was created for LauzHack 2025.
+1. Fork the repository
+2. Create a feature branch
+3. Make your changes
+4. Submit a pull request
diff --git a/REFACTORING_SUMMARY.md b/REFACTORING_SUMMARY.md
deleted file mode 100644
index 98b0494..0000000
--- a/REFACTORING_SUMMARY.md
+++ /dev/null
@@ -1,223 +0,0 @@
-# Refactoring Summary
-
-This document outlines all the changes made during the code refactoring to separate backend and frontend.
-
-## New Project Structure
-
-```
-lauzhack-2025/
-â”œâ”€â”€ backend/                    # Backend application
-â”‚   â”œâ”€â”€ app/
-â”‚   â”‚   â”œâ”€â”€ api/
-â”‚   â”‚   â”‚   â”œâ”€â”€ main.py        # FastAPI app (moved from root/main.py)
-â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â”‚   â”œâ”€â”€ core/
-â”‚   â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration (moved from root/config.py)
-â”‚   â”‚   â”‚   â”œâ”€â”€ state_manager.py # State management (moved from root/state_manager.py)
-â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â”‚   â”œâ”€â”€ engines/
-â”‚   â”‚   â”‚   â”œâ”€â”€ fact_engine.py  # Fact checking (moved from root/fact_engine.py)
-â”‚   â”‚   â”‚   â”œâ”€â”€ topic_engine.py # Topic tracking (moved from root/topic_engine.py)
-â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â”‚   â”œâ”€â”€ utils/
-â”‚   â”‚   â”‚   â”œâ”€â”€ logger_util.py  # Logging (moved from root/logger_util.py)
-â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â”œâ”€â”€ tests/
-â”‚   â”‚   â”œâ”€â”€ test_wav_stream.py  # Test client (moved from root/test_wav_stream.py)
-â”‚   â”‚   â”œâ”€â”€ test_audio/         # Test audio files (moved from root/test_audio/)
-â”‚   â”‚   â””â”€â”€ test_data/          # Test data (moved from root/test_data/)
-â”‚   â”œâ”€â”€ logs/                   # Application logs (moved from root/logs/)
-â”‚   â”œâ”€â”€ run.py                  # NEW: Backend entry point
-â”‚   â”œâ”€â”€ __main__.py             # NEW: Allow running as module
-â”‚   â”œâ”€â”€ requirements.txt        # Dependencies (moved from root/requirements.txt)
-â”‚   â”œâ”€â”€ .env                    # Environment variables (copied from root/.env)
-â”‚   â”œâ”€â”€ .env.example            # NEW: Example environment file
-â”‚   â””â”€â”€ README.md               # NEW: Backend documentation
-â”œâ”€â”€ frontend/                   # NEW: Frontend structure
-â”‚   â”œâ”€â”€ src/
-â”‚   â”‚   â”œâ”€â”€ components/
-â”‚   â”‚   â”œâ”€â”€ pages/
-â”‚   â”‚   â”œâ”€â”€ styles/
-â”‚   â”‚   â”œâ”€â”€ utils/
-â”‚   â”‚   â””â”€â”€ assets/
-â”‚   â”œâ”€â”€ public/
-â”‚   â”œâ”€â”€ package.json            # NEW: Placeholder package.json
-â”‚   â””â”€â”€ README.md               # NEW: Frontend documentation
-â”œâ”€â”€ docs/                       # Documentation
-â”‚   â”œâ”€â”€ guides/                 # Moved from "feature summarization and guides/"
-â”‚   â”œâ”€â”€ CLAUDE.md               # Moved from root/
-â”‚   â””â”€â”€ README.md               # Moved from root/
-â”œâ”€â”€ .env                        # Kept in root for compatibility
-â”œâ”€â”€ .gitignore                  # Updated with new structure
-â”œâ”€â”€ LICENSE
-â”œâ”€â”€ README.md                   # NEW: Main project README
-â””â”€â”€ venv/                       # Python virtual environment
-```
-
-## Changes Made
-
-### 1. File Moves and Reorganization
-
-**Backend Files:**
-- `main.py` â†’ `backend/app/api/main.py`
-- `config.py` â†’ `backend/app/core/config.py`
-- `state_manager.py` â†’ `backend/app/core/state_manager.py`
-- `fact_engine.py` â†’ `backend/app/engines/fact_engine.py`
-- `topic_engine.py` â†’ `backend/app/engines/topic_engine.py`
-- `logger_util.py` â†’ `backend/app/utils/logger_util.py`
-- `test_wav_stream.py` â†’ `backend/tests/test_wav_stream.py`
-- `requirements.txt` â†’ `backend/requirements.txt`
-- `test_audio/` â†’ `backend/tests/test_audio/`
-- `test_data/` â†’ `backend/tests/test_data/`
-- `logs/` â†’ `backend/logs/`
-
-**Documentation:**
-- `"feature summarization and guides/"` â†’ `docs/guides/`
-- `CLAUDE.md` â†’ `docs/CLAUDE.md`
-- Original `README.md` â†’ `docs/README.md`
-
-### 2. Import Updates
-
-All Python files have been updated to use the new import paths:
-
-**Example: backend/app/api/main.py**
-```python
-# OLD:
-from config import settings
-from state_manager import state, TranscriptSegment
-from topic_engine import topic_engine
-from fact_engine import fact_engine
-
-# NEW:
-from backend.app.core.config import settings
-from backend.app.core.state_manager import state, TranscriptSegment
-from backend.app.engines.topic_engine import topic_engine
-from backend.app.engines.fact_engine import fact_engine
-```
-
-**Example: backend/app/engines/fact_engine.py**
-```python
-# OLD:
-from config import settings, CLAIM_DETECTION_PROMPT, ...
-from state_manager import state, FactCheckResult
-from logger_util import debug_logger
-
-# NEW:
-from backend.app.core.config import settings, CLAIM_DETECTION_PROMPT, ...
-from backend.app.core.state_manager import state, FactCheckResult
-from backend.app.utils.logger_util import debug_logger
-```
-
-### 3. Configuration Updates
-
-**backend/app/core/config.py:**
-- Updated to load `.env` file from `backend/.env`
-- Added path resolution: `ENV_FILE = BACKEND_DIR / ".env"`
-
-**backend/tests/test_wav_stream.py:**
-- Updated to use relative paths for test data
-- Changed from absolute paths to: `TEST_DATA_DIR = SCRIPT_DIR / "test_data"`
-
-### 4. New Files Created
-
-1. **backend/run.py** - Entry point to run the FastAPI server
-2. **backend/__main__.py** - Allows running with `python -m backend`
-3. **backend/.env.example** - Example environment variables template
-4. **backend/README.md** - Comprehensive backend documentation
-5. **frontend/README.md** - Frontend structure documentation
-6. **frontend/package.json** - Placeholder package.json
-7. **README.md** - Main project documentation (root)
-
-### 5. .gitignore Updates
-
-Added:
-- `logs/` directory
-- Frontend-specific ignores (node_modules, dist, build, etc.)
-
-## How to Run the Backend
-
-### Option 1: Using run.py
-```bash
-cd backend
-python run.py
-```
-
-### Option 2: As a Python module
-```bash
-python -m backend
-```
-
-### Option 3: Using uvicorn directly
-```bash
-uvicorn backend.app.api.main:app --host 0.0.0.0 --port 8765 --reload
-```
-
-## Testing
-
-To run the WAV streaming test:
-```bash
-cd backend/tests
-python test_wav_stream.py
-```
-
-Make sure the backend server is running first.
-
-## Environment Setup
-
-1. Copy the example environment file:
-   ```bash
-   cd backend
-   cp .env.example .env
-   ```
-
-2. Edit `.env` and add your API keys:
-   - `DEEPGRAM_API_KEY`
-   - `TOGETHER_API_KEY`
-
-3. Install dependencies:
-   ```bash
-   pip install -r requirements.txt
-   ```
-
-## Breaking Changes
-
-### Import Paths
-All imports have changed. If you have any custom scripts or tests, update them to use the new import paths.
-
-### File Locations
-- Test data is now in `backend/tests/test_data/`
-- Logs are now in `backend/logs/`
-- Configuration is now in `backend/.env`
-
-### Running the Application
-You must now run the application from the project root using one of the methods above.
-
-## Benefits of This Structure
-
-1. **Clear Separation**: Backend and frontend are completely separated
-2. **Scalability**: Easy to add new features without cluttering the root
-3. **Professional Structure**: Follows industry best practices
-4. **Easy Deployment**: Backend can be containerized independently
-5. **Team Collaboration**: Frontend and backend teams can work independently
-6. **Testing**: Test files are organized with the code they test
-
-## Next Steps
-
-1. **Test the Backend**: Run the server and verify all endpoints work
-2. **Update CI/CD**: If you have CI/CD pipelines, update them for the new structure
-3. **Implement Frontend**: Use the frontend/ structure when implementing from Figma
-4. **Documentation**: Update any additional documentation with new paths
-
-## Rollback (if needed)
-
-If you need to rollback:
-1. The original files were deleted from root, but git history has them
-2. Use `git log` to find the commit before refactoring
-3. Use `git checkout <commit-hash> -- <file>` to restore specific files
-
-## Questions or Issues?
-
-- Check `backend/README.md` for backend-specific documentation
-- Check `frontend/README.md` for frontend structure
-- See `docs/` directory for detailed guides
diff --git a/UPDATE_SUMMARY.md b/UPDATE_SUMMARY.md
new file mode 100644
index 0000000..16bdb96
--- /dev/null
+++ b/UPDATE_SUMMARY.md
@@ -0,0 +1,175 @@
+# Update Summary - Dependencies Fixed âœ“
+
+## Changes Made
+
+### 1. Updated Requirements
+Fixed version compatibility issues in `requirements.txt`:
+
+- **Deepgram SDK**: `3.8.3` â†’ `5.3.0` âœ“
+- **DuckDuckGo Search**: `7.0.0` â†’ `8.1.1` âœ“
+
+All other dependencies remain unchanged and compatible.
+
+### 2. Code Updates
+
+#### main.py
+Updated Deepgram SDK integration for v5.x compatibility:
+
+```python
+# Removed obsolete imports
+- DeepgramClientOptions  # No longer needed in 5.x
+
+# Simplified client initialization
+- config = DeepgramClientOptions(options={"keepalive": "true"})
+- deepgram = DeepgramClient(api_key, config)
++ deepgram = DeepgramClient(api_key)
+
+# Updated connection path
+- deepgram.listen.asynclive.v("1")
++ deepgram.listen.live.v("1")
+
+# Fixed options format
+- utterance_end_ms=1000
++ utterance_end_ms="1000"
+```
+
+#### fact_engine.py
+No changes needed - DuckDuckGo Search API remains compatible.
+
+### 3. New Files Added
+
+- **verify_setup.py**: Automated setup verification script
+- **MIGRATION_NOTES.md**: Detailed migration guide
+- **UPDATE_SUMMARY.md**: This file
+
+## Installation Steps
+
+```bash
+# 1. Install dependencies (now works!)
+pip install -r requirements.txt
+
+# 2. Set up environment
+cp .env.example .env
+# Edit .env and add your API keys
+
+# 3. Verify setup
+python verify_setup.py
+
+# 4. Run the server
+python main.py
+
+# 5. Test (optional)
+python test_client.py
+```
+
+## Verification
+
+Run the verification script to ensure everything is set up correctly:
+
+```bash
+python verify_setup.py
+```
+
+This checks:
+- âœ“ All package imports
+- âœ“ Deepgram SDK version (5.x)
+- âœ“ Environment configuration
+- âœ“ Custom module imports
+- âœ“ API compatibility
+
+## What's Working
+
+All features remain fully functional:
+
+- âœ“ FastAPI WebSocket server
+- âœ“ Deepgram live transcription
+- âœ“ Dual-loop architecture (Fast + Slow)
+- âœ“ Topic tracking with NetworkX
+- âœ“ 3-step fact-checking pipeline
+- âœ“ Source link tracking for evidence
+- âœ“ Async/await non-blocking design
+- âœ“ Rate limiting
+- âœ“ REST API endpoints
+
+## Testing
+
+### Quick Test (without audio)
+```bash
+python test_client.py
+```
+
+### Check Server Status
+```bash
+curl http://localhost:8000/
+curl http://localhost:8000/stats
+curl http://localhost:8000/topics
+curl http://localhost:8000/facts
+```
+
+## Key Improvements
+
+1. **Latest Stable Versions**: Using current stable releases
+2. **Better Async Support**: Deepgram 5.x has improved async handling
+3. **Simplified API**: Cleaner initialization code
+4. **Automated Verification**: New script to check setup
+5. **Better Documentation**: Migration notes and version info
+
+## Compatibility
+
+- **Python**: 3.8+ required
+- **OS**: Windows, Linux, macOS
+- **All dependencies**: Verified compatible
+
+## Next Steps
+
+1. Install dependencies: `pip install -r requirements.txt`
+2. Configure `.env` with your API keys
+3. Run verification: `python verify_setup.py`
+4. Start coding! ğŸš€
+
+## Files Overview
+
+```
+lauzhack-2025/
+â”œâ”€â”€ Core Application
+â”‚   â”œâ”€â”€ main.py              # FastAPI server
+â”‚   â”œâ”€â”€ config.py            # Configuration
+â”‚   â”œâ”€â”€ state_manager.py     # State management
+â”‚   â”œâ”€â”€ topic_engine.py      # Fast loop
+â”‚   â””â”€â”€ fact_engine.py       # Slow loop
+â”‚
+â”œâ”€â”€ Testing & Verification
+â”‚   â”œâ”€â”€ test_client.py       # Test without audio
+â”‚   â””â”€â”€ verify_setup.py      # Setup checker
+â”‚
+â”œâ”€â”€ Documentation
+â”‚   â”œâ”€â”€ README.md            # Complete guide
+â”‚   â”œâ”€â”€ QUICKSTART.md        # Quick start
+â”‚   â”œâ”€â”€ MIGRATION_NOTES.md   # API changes
+â”‚   â””â”€â”€ UPDATE_SUMMARY.md    # This file
+â”‚
+â””â”€â”€ Configuration
+    â”œâ”€â”€ requirements.txt     # Dependencies (updated!)
+    â”œâ”€â”€ .env.example         # Environment template
+    â””â”€â”€ .gitignore          # Git ignore rules
+```
+
+## Support
+
+If you encounter issues:
+
+1. Run `python verify_setup.py` to diagnose
+2. Check `MIGRATION_NOTES.md` for API changes
+3. Review error logs for specific issues
+4. Ensure API keys are correctly configured in `.env`
+
+## Success Criteria
+
+When `verify_setup.py` shows all checks passing (âœ“), you're ready to go!
+
+---
+
+**Status**: âœ“ All dependencies fixed and compatible
+**Last Updated**: 2025-11-22
+**Deepgram SDK**: 5.3.0
+**DuckDuckGo Search**: 8.1.1
diff --git a/docs/guides/WAV_STREAMING_GUIDE.md b/WAV_STREAMING_GUIDE.md
similarity index 100%
rename from docs/guides/WAV_STREAMING_GUIDE.md
rename to WAV_STREAMING_GUIDE.md
diff --git a/backend/.env.example b/backend/.env.example
deleted file mode 100644
index 4adccf5..0000000
--- a/backend/.env.example
+++ /dev/null
@@ -1,8 +0,0 @@
-# API Keys - Copy this to .env and fill in your actual keys
-DEEPGRAM_API_KEY=your_deepgram_api_key_here
-TOGETHER_API_KEY=your_together_api_key_here
-# HUGGINGFACE_API_KEY=your_huggingface_api_key_here
-
-# Configuration
-FACT_CHECK_RATE_LIMIT=10  # seconds between fact checks
-TOPIC_UPDATE_THRESHOLD=15  # finalized sentences before topic update (increased to reduce irrelevant topics)
diff --git a/backend/README.md b/backend/README.md
deleted file mode 100644
index 63dc185..0000000
--- a/backend/README.md
+++ /dev/null
@@ -1,166 +0,0 @@
-# Backend - Real-Time Podcast AI Assistant
-
-FastAPI-based backend for real-time podcast transcription, topic tracking, and fact-checking.
-
-## Structure
-
-```
-backend/
-â”œâ”€â”€ app/
-â”‚   â”œâ”€â”€ api/
-â”‚   â”‚   â”œâ”€â”€ main.py          # FastAPI app and WebSocket endpoints
-â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â”œâ”€â”€ core/
-â”‚   â”‚   â”œâ”€â”€ config.py        # Configuration and settings
-â”‚   â”‚   â”œâ”€â”€ state_manager.py # Global state management
-â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â”œâ”€â”€ engines/
-â”‚   â”‚   â”œâ”€â”€ fact_engine.py   # Fact-checking pipeline
-â”‚   â”‚   â”œâ”€â”€ topic_engine.py  # Topic detection and tracking
-â”‚   â”‚   â””â”€â”€ __init__.py
-â”‚   â””â”€â”€ utils/
-â”‚       â”œâ”€â”€ logger_util.py   # Logging utilities
-â”‚       â””â”€â”€ __init__.py
-â”œâ”€â”€ tests/
-â”‚   â”œâ”€â”€ test_wav_stream.py   # WAV streaming test client
-â”‚   â”œâ”€â”€ test_audio/          # Test audio files
-â”‚   â””â”€â”€ test_data/           # Test data files
-â”œâ”€â”€ logs/                     # Application logs
-â”œâ”€â”€ run.py                    # Entry point to run the server
-â”œâ”€â”€ __main__.py              # Allow running as module
-â”œâ”€â”€ requirements.txt         # Python dependencies
-â”œâ”€â”€ .env                     # Environment variables (create from .env.example)
-â””â”€â”€ .env.example             # Example environment variables
-```
-
-## Installation
-
-1. Create a virtual environment (recommended):
-   ```bash
-   python -m venv venv
-   source venv/bin/activate  # On Windows: venv\Scripts\activate
-   ```
-
-2. Install dependencies:
-   ```bash
-   cd backend
-   pip install -r requirements.txt
-   ```
-
-3. Set up environment variables:
-   ```bash
-   cp .env.example .env
-   ```
-
-   Edit `.env` and add your API keys:
-   - `DEEPGRAM_API_KEY`: Your Deepgram API key
-   - `TOGETHER_API_KEY`: Your Together AI API key
-
-## Running the Server
-
-### Method 1: Using run.py
-```bash
-python run.py
-```
-
-### Method 2: As a module
-```bash
-python -m backend
-```
-
-### Method 3: Using uvicorn directly
-```bash
-uvicorn backend.app.api.main:app --host 0.0.0.0 --port 8765 --reload
-```
-
-The server will start on `http://localhost:8765`
-
-## API Endpoints
-
-### WebSocket Endpoint
-- **URL**: `ws://localhost:8765/ws`
-- **Protocol**: Audio streaming + JSON messages
-
-#### Client â†’ Server Messages
-1. **Audio Frames**: Raw audio bytes (16kHz, 16-bit, mono PCM)
-2. **Control Messages**: JSON objects for control
-
-#### Server â†’ Client Messages
-1. **Transcript Updates**: Real-time transcription results
-2. **Topic Updates**: Topic tree changes
-3. **Fact Check Results**: Verification results
-
-### HTTP Endpoints
-- `GET /`: Root endpoint with API info
-- `GET /health`: Health check endpoint
-
-## Testing
-
-### WAV File Streaming Test
-Tests the WebSocket connection by streaming a WAV file:
-
-```bash
-cd backend/tests
-python test_wav_stream.py
-```
-
-Make sure the backend server is running before starting the test.
-
-## Configuration
-
-Key configuration options in `.env`:
-
-- `FACT_CHECK_RATE_LIMIT`: Seconds between fact checks (default: 10)
-- `TOPIC_UPDATE_THRESHOLD`: Sentences before topic update (default: 15)
-- `TOGETHER_MODEL`: LLM model to use (default: Meta-Llama-3.1-70B-Instruct-Turbo)
-
-## Architecture
-
-### Three-Loop System
-
-1. **Fast Loop (WebSocket)**: Real-time transcription
-2. **Medium Loop (Topic Engine)**: Topic detection every N sentences
-3. **Slow Loop (Fact Engine)**: Background fact-checking queue
-
-### State Management
-Global state is managed in `state_manager.py`:
-- Transcript buffer (circular buffer)
-- Topic tree (NetworkX graph)
-- Fact-checking queue and results
-
-## Development
-
-### Adding New Features
-
-1. **New API Endpoint**: Add to `app/api/main.py`
-2. **New Configuration**: Add to `app/core/config.py`
-3. **New Engine**: Create in `app/engines/`
-4. **New Utility**: Add to `app/utils/`
-
-### Logging
-Logs are written to `backend/logs/` directory. Use the `debug_logger` from `utils/logger_util.py` for debug logging.
-
-## Troubleshooting
-
-### Import Errors
-Make sure you're running from the project root and that the project root is in your PYTHONPATH.
-
-### API Key Errors
-Verify your `.env` file has valid API keys for Deepgram and Together AI.
-
-### WebSocket Connection Issues
-- Check that the server is running on port 8765
-- Verify firewall settings
-- Check server logs in `backend/logs/`
-
-## Dependencies
-
-Main dependencies:
-- FastAPI: Web framework
-- Uvicorn: ASGI server
-- Deepgram SDK: Audio transcription
-- Together AI: LLM inference
-- NetworkX: Graph operations for topic tree
-- DuckDuckGo Search: Web search for fact-checking
-
-See `requirements.txt` for complete list.
diff --git a/backend/STREAMING_API.md b/backend/STREAMING_API.md
deleted file mode 100644
index 9435b2a..0000000
--- a/backend/STREAMING_API.md
+++ /dev/null
@@ -1,282 +0,0 @@
-# Streaming API Documentation
-
-Server-side audio streaming API for MVP demo. Processes a hardcoded audio file and provides results via JSON file and REST endpoints.
-
-## Overview
-
-This API allows you to:
-1. Start processing a pre-recorded audio file server-side
-2. Monitor processing progress in real-time
-3. Access results incrementally as they're generated
-4. Stop processing at any time
-
-## Architecture
-
-```
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚   Client    â”‚
-â”‚  (Frontend) â”‚
-â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
-       â”‚
-       â”‚ HTTP REST API
-       â”‚
-â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚              FastAPI Backend                     â”‚
-â”‚                                                  â”‚
-â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
-â”‚  â”‚      StreamProcessor                    â”‚    â”‚
-â”‚  â”‚  - Reads hardcoded WAV file            â”‚    â”‚
-â”‚  â”‚  - Streams to Deepgram                 â”‚    â”‚
-â”‚  â”‚  - Processes transcripts, topics, factsâ”‚    â”‚
-â”‚  â”‚  - Writes to stream_output.json        â”‚    â”‚
-â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
-â”‚                                                  â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-       â”‚
-       â”‚ Writes to
-       â–¼
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚stream_output.jsonâ”‚  â† Frontend can read this file
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    or use /api/stream/results
-```
-
-## API Endpoints
-
-### POST /api/stream/start
-
-Start streaming the hardcoded audio file.
-
-**Request:**
-```bash
-curl -X POST http://localhost:8000/api/stream/start
-```
-
-**Response:**
-```json
-{
-  "status": "started",
-  "file": "/path/to/audio/file.wav"
-}
-```
-
-**Behavior:**
-- Starts processing audio in chunks (100ms intervals)
-- Simulates real-time streaming
-- Runs transcription, topic detection, and fact-checking
-- Writes results incrementally to `stream_output.json`
-
-### POST /api/stream/stop
-
-Stop the current streaming session.
-
-**Request:**
-```bash
-curl -X POST http://localhost:8000/api/stream/stop
-```
-
-**Response:**
-```json
-{
-  "status": "stopped"
-}
-```
-
-### GET /api/stream/status
-
-Get current streaming status.
-
-**Request:**
-```bash
-curl http://localhost:8000/api/stream/status
-```
-
-**Response:**
-```json
-{
-  "is_streaming": true,
-  "status": "streaming",
-  "progress": 45.2,
-  "file": "/path/to/LexNuclear.wav",
-  "transcripts_count": 150,
-  "topics_count": 5,
-  "fact_checks_count": 12
-}
-```
-
-### GET /api/stream/results
-
-Get complete current results.
-
-**Request:**
-```bash
-curl http://localhost:8000/api/stream/results
-```
-
-**Response:**
-```json
-{
-  "status": "streaming",
-  "started_at": "2025-11-23T10:30:00",
-  "progress": 45.2,
-  "transcripts": [
-    {
-      "text": "Hello world",
-      "is_final": true,
-      "confidence": 0.95,
-      "timestamp": "2025-11-23T10:30:05"
-    }
-  ],
-  "topics": [
-    {
-      "topic_id": "node_0",
-      "topic": "Introduction to AI",
-      "total_topics": 1,
-      "timestamp": "2025-11-23T10:30:10"
-    }
-  ],
-  "fact_checks": [
-    {
-      "claim": "The Earth revolves around the Sun",
-      "verdict": "SUPPORTED",
-      "confidence": 0.98,
-      "explanation": "...",
-      "key_facts": [...],
-      "sources": [...],
-      "timestamp": "2025-11-23T10:30:15"
-    }
-  ],
-  "metadata": {
-    "filename": "LexNuclear.wav",
-    "channels": 2,
-    "sample_width": 16,
-    "framerate": 48000,
-    "total_frames": 14400000,
-    "duration_seconds": 300.0
-  }
-}
-```
-
-## Output File
-
-Results are written to `backend/stream_output.json` incrementally. This file is updated:
-- When new transcript segments arrive (every ~100ms)
-- When new topics are detected
-- When fact-checks complete
-- Progress updates every 5%
-
-## Usage Example
-
-### Starting a Stream
-
-```bash
-# 1. Start the stream
-curl -X POST http://localhost:8000/api/stream/start
-
-# 2. Check status periodically
-curl http://localhost:8000/api/stream/status
-
-# 3. Get results (or read stream_output.json directly)
-curl http://localhost:8000/api/stream/results
-```
-
-### For Frontend Integration
-
-**Option 1: Poll the API**
-```javascript
-// Start stream
-await fetch('http://localhost:8000/api/stream/start', { method: 'POST' });
-
-// Poll for updates every second
-const interval = setInterval(async () => {
-  const response = await fetch('http://localhost:8000/api/stream/results');
-  const data = await response.json();
-
-  // Update UI with data
-  updateUI(data);
-
-  // Stop polling when complete
-  if (data.status === 'complete') {
-    clearInterval(interval);
-  }
-}, 1000);
-```
-
-**Option 2: Read the JSON file directly**
-```javascript
-// If serving the JSON file statically
-const interval = setInterval(async () => {
-  const response = await fetch('/stream_output.json');
-  const data = await response.json();
-  updateUI(data);
-}, 1000);
-```
-
-## Configuration
-
-Hardcoded audio file location:
-```python
-# backend/app/services/stream_processor.py
-DEFAULT_AUDIO_FILE = Path(__file__).parent.parent.parent / "tests" / "test_data" / "LexNuclear.wav"
-```
-
-Streaming parameters (same as test_wav_stream.py):
-```python
-CHUNK_DURATION_MS = 100  # Send chunks every 100ms
-```
-
-## Processing Flow
-
-1. **Audio Chunking**: Reads WAV file in 100ms chunks
-2. **Deepgram Transcription**: Sends chunks to Deepgram API
-3. **Transcript Processing**:
-   - Partial transcripts â†’ stored temporarily
-   - Final transcripts â†’ added to results, triggers topic/fact processing
-4. **Topic Detection**: Every N sentences, extracts topics
-5. **Fact Checking**: Queued for background processing, results monitored
-6. **JSON Writing**: Updates file after each significant event
-
-## Code Reuse
-
-This implementation **reuses code** from:
-- `backend/app/api/main.py` - WebSocket endpoint logic
-- `backend/tests/test_wav_stream.py` - Audio streaming logic
-- Existing engines (topic_engine, fact_engine) unchanged
-
-This ensures:
-- Consistency with proven working code
-- Easy to merge future improvements
-- Minimal code duplication
-
-## Differences from WebSocket Endpoint
-
-| Aspect | WebSocket `/listen` | Streaming API |
-|--------|-------------------|---------------|
-| Client | External (test script) | Server-side |
-| Audio Source | Uploaded/streamed | Hardcoded file |
-| Output | WebSocket messages | JSON file + REST API |
-| Use Case | Live recording | MVP demo |
-
-## Future Improvements
-
-- [ ] Support custom audio file selection
-- [ ] Add WebSocket for real-time updates (instead of polling)
-- [ ] Add pause/resume functionality
-- [ ] Support multiple concurrent streams
-- [ ] Add audio playback synchronization timestamps
-
-## Troubleshooting
-
-**Stream won't start:**
-- Check audio file exists at the hardcoded path
-- Verify Deepgram API key is configured
-- Check backend logs for errors
-
-**Results not updating:**
-- Verify `stream_output.json` is being written
-- Check file permissions
-- Monitor backend logs for processing errors
-
-**Fact-checks not appearing:**
-- Fact-checking runs in background and may take time
-- Monitor `/api/stream/status` for fact_checks_count
-- Check that fact_engine background task is running
diff --git a/backend/__init__.py b/backend/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/backend/__main__.py b/backend/__main__.py
deleted file mode 100644
index 4898673..0000000
--- a/backend/__main__.py
+++ /dev/null
@@ -1,20 +0,0 @@
-"""
-Allow running backend as a module: python -m backend
-"""
-
-import uvicorn
-import sys
-from pathlib import Path
-
-# Add the project root to the path to allow imports
-project_root = Path(__file__).parent.parent
-sys.path.insert(0, str(project_root))
-
-if __name__ == "__main__":
-    uvicorn.run(
-        "backend.app.api.main:app",
-        host="0.0.0.0",
-        port=8000,
-        reload=True,
-        log_level="info"
-    )
diff --git a/backend/app/__init__.py b/backend/app/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/backend/app/api/__init__.py b/backend/app/api/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/backend/app/core/__init__.py b/backend/app/core/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/backend/app/engines/__init__.py b/backend/app/engines/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/backend/app/services/__init__.py b/backend/app/services/__init__.py
deleted file mode 100644
index f77022e..0000000
--- a/backend/app/services/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-"""Services module for backend processing."""
-
-from backend.app.services.stream_processor import stream_processor
-
-__all__ = ['stream_processor']
diff --git a/backend/app/services/stream_processor.py b/backend/app/services/stream_processor.py
deleted file mode 100644
index c61e81b..0000000
--- a/backend/app/services/stream_processor.py
+++ /dev/null
@@ -1,521 +0,0 @@
-"""
-Server-Side Stream Processor
-Reuses the working WebSocket logic to process audio files server-side
-and write results to JSON for frontend consumption.
-"""
-
-import asyncio
-import json
-import logging
-import wave
-from datetime import datetime
-from pathlib import Path
-from typing import Dict, Any, Optional, List
-from deepgram import AsyncDeepgramClient
-from deepgram.core.events import EventType
-
-from backend.app.core.config import settings
-from backend.app.core.state_manager import state, TranscriptSegment
-from backend.app.engines.topic_engine import topic_engine
-from backend.app.engines.fact_engine import fact_engine
-from backend.app.utils.logger_util import DebugLogger
-
-logger = logging.getLogger(__name__)
-
-# Streaming configuration - same as test_wav_stream.py
-CHUNK_DURATION_MS = 100  # Send chunks every 100ms (simulates real-time)
-DEFAULT_AUDIO_FILE = Path(__file__).parent.parent.parent / "tests" / "test_data" / "LexNuclear.wav"
-RESULTS_FILE = Path(__file__).parent.parent.parent / "stream_output.json"
-
-
-class StreamProcessor:
-    """
-    Processes audio files server-side using the same logic as the WebSocket endpoint.
-    Writes results incrementally to a JSON file instead of sending via WebSocket.
-    """
-
-    def __init__(self, audio_file: Optional[Path] = None):
-        self.audio_file = audio_file or DEFAULT_AUDIO_FILE
-        self.is_streaming = False
-        self.stream_task: Optional[asyncio.Task] = None
-        self.fact_monitor_task: Optional[asyncio.Task] = None
-        self.last_fact_check_count = 0
-        self.session_logger: Optional[DebugLogger] = None
-        self.session_start_time: Optional[datetime] = None
-        self.results: Dict[str, Any] = {
-            "status": "idle",
-            "started_at": None,
-            "progress": 0.0,
-            "transcripts": [],
-            "topics": [],
-            "fact_checks": [],
-            "metadata": {}
-        }
-        self._reset_state()
-
-    def _reset_state(self):
-        """Reset application state for new stream."""
-        # Clear the state manager buffers
-        state.transcript_buffer.clear()
-        state.fact_results.clear()
-        state.finalized_sentence_count = 0
-        state.current_topic_id = None
-        self.last_fact_check_count = 0
-
-    async def start_stream(self) -> Dict[str, Any]:
-        """Start processing the audio file."""
-        if self.is_streaming:
-            return {"error": "Stream already running"}
-
-        if not self.audio_file.exists():
-            return {"error": f"Audio file not found: {self.audio_file}"}
-
-        # Reset state
-        self._reset_state()
-        
-        # Initialize new session logger
-        self.session_logger = DebugLogger()
-        self.session_start_time = datetime.now()
-        
-        self.results = {
-            "status": "starting",
-            "started_at": self.session_start_time.isoformat(),
-            "progress": 0.0,
-            "transcripts": [],
-            "topics": [],
-            "fact_checks": [],
-            "metadata": {}
-        }
-
-        # Get audio file metadata
-        with wave.open(str(self.audio_file), 'rb') as wav_file:
-            self.results["metadata"] = {
-                "filename": self.audio_file.name,
-                "channels": wav_file.getnchannels(),
-                "sample_width": wav_file.getsampwidth() * 8,  # bits
-                "framerate": wav_file.getframerate(),
-                "total_frames": wav_file.getnframes(),
-                "duration_seconds": wav_file.getnframes() / wav_file.getframerate()
-            }
-
-        # Write initial state
-        self._write_results()
-
-        # Start streaming task
-        self.stream_task = asyncio.create_task(self._process_stream())
-
-        # Start fact-check monitor task
-        self.fact_monitor_task = asyncio.create_task(self._monitor_fact_checks())
-
-        self.is_streaming = True
-
-        logger.info(f"Started streaming: {self.audio_file}")
-        return {"status": "started", "file": str(self.audio_file)}
-
-    async def stop_stream(self) -> Dict[str, Any]:
-        """Stop the current stream."""
-        if not self.is_streaming:
-            return {"error": "No stream running"}
-
-        self.is_streaming = False
-
-        if self.stream_task:
-            self.stream_task.cancel()
-            try:
-                await self.stream_task
-            except asyncio.CancelledError:
-                pass
-
-        if self.fact_monitor_task:
-            self.fact_monitor_task.cancel()
-            try:
-                await self.fact_monitor_task
-            except asyncio.CancelledError:
-                pass
-
-        self.results["status"] = "stopped"
-        self._write_results()
-        
-        # Save session data to logs
-        self.save_session()
-
-        logger.info("Stream stopped")
-        return {"status": "stopped"}
-
-    def get_status(self) -> Dict[str, Any]:
-        """Get current stream status."""
-        return {
-            "is_streaming": self.is_streaming,
-            "status": self.results.get("status"),
-            "progress": self.results.get("progress", 0),
-            "file": str(self.audio_file),
-            "transcripts_count": len(self.results.get("transcripts", [])),
-            "topics_count": len(self.results.get("topics", [])),
-            "fact_checks_count": len(self.results.get("fact_checks", []))
-        }
-
-    def get_results(self) -> Dict[str, Any]:
-        """Get current results."""
-        return self.results.copy()
-
-    def save_session(self):
-        """
-        Save complete session data to a session-specific directory.
-        Creates a comprehensive session record including all transcripts, topics, 
-        fact checks, and metadata in the session's own folder.
-        """
-        if not self.session_logger:
-            logger.warning("No session logger available - session not saved")
-            return
-        
-        try:
-            # Save the DebugLogger's built-in session summary
-            self.session_logger.save_summary()
-            
-            # Also save a comprehensive combined session file in the session directory
-            session_end_time = datetime.now()
-            session_duration = (session_end_time - self.session_start_time).total_seconds() if self.session_start_time else 0
-            
-            comprehensive_session = {
-                "session_id": self.session_logger.session_id,
-                "session_directory": str(self.session_logger.session_dir),
-                "started_at": self.session_start_time.isoformat() if self.session_start_time else None,
-                "completed_at": session_end_time.isoformat(),
-                "duration_seconds": session_duration,
-                "status": self.results.get("status", "unknown"),
-                "audio_file": {
-                    "filename": self.results.get("metadata", {}).get("filename", str(self.audio_file.name)),
-                    "path": str(self.audio_file),
-                    "metadata": self.results.get("metadata", {})
-                },
-                "statistics": {
-                    "total_transcripts": len(self.results.get("transcripts", [])),
-                    "final_transcripts": sum(1 for t in self.results.get("transcripts", []) if t.get("is_final", False)),
-                    "total_topics": len(self.results.get("topics", [])),
-                    "total_fact_checks": len(self.results.get("fact_checks", [])),
-                    "fact_verdicts": {
-                        "SUPPORTED": sum(1 for f in self.results.get("fact_checks", []) if f.get("verdict") == "SUPPORTED"),
-                        "CONTRADICTED": sum(1 for f in self.results.get("fact_checks", []) if f.get("verdict") == "CONTRADICTED"),
-                        "UNCERTAIN": sum(1 for f in self.results.get("fact_checks", []) if f.get("verdict") == "UNCERTAIN"),
-                    }
-                },
-                "transcripts": self.results.get("transcripts", []),
-                "topics": self.results.get("topics", []),
-                "fact_checks": self.results.get("fact_checks", [])
-            }
-            
-            # Save comprehensive file to the session directory
-            comprehensive_file = self.session_logger.session_dir / "complete_session.json"
-            
-            with open(comprehensive_file, 'w') as f:
-                json.dump(comprehensive_session, f, indent=2)
-            
-            logger.info(f"Session saved successfully to: {self.session_logger.session_dir}")
-            logger.info(f"Session stats: {comprehensive_session['statistics']['total_transcripts']} transcripts, "
-                       f"{comprehensive_session['statistics']['total_topics']} topics, "
-                       f"{comprehensive_session['statistics']['total_fact_checks']} fact checks")
-            
-        except Exception as e:
-            logger.error(f"Failed to save session: {e}", exc_info=True)
-
-    def _write_results(self):
-        """Write current results to JSON file."""
-        try:
-            with open(RESULTS_FILE, 'w') as f:
-                json.dump(self.results, f, indent=2)
-        except Exception as e:
-            logger.error(f"Failed to write results: {e}")
-
-    async def _process_stream(self):
-        """
-        Main streaming logic - reuses the WebSocket endpoint logic
-        but writes to JSON instead of sending via WebSocket.
-        """
-        try:
-            self.results["status"] = "streaming"
-            self._write_results()
-
-            # Initialize Deepgram client - same as WebSocket endpoint
-            deepgram = AsyncDeepgramClient(api_key=settings.deepgram_api_key)
-
-            # Configure Deepgram - same as WebSocket endpoint
-            options = {
-                "model": "nova-3",
-                "encoding": "linear16",
-                "sample_rate": "48000",
-                "channels": "1",
-            }
-
-            async with deepgram.listen.v1.connect(**options) as dg_connection:
-                logger.info("Deepgram connection established")
-
-                # Event handlers - adapted from WebSocket endpoint
-                def on_message(message):
-                    """Handle incoming transcription from Deepgram."""
-                    try:
-                        if hasattr(message, 'channel') and hasattr(message.channel, 'alternatives'):
-                            alternatives = message.channel.alternatives
-                            if len(alternatives) > 0 and hasattr(alternatives[0], 'transcript'):
-                                sentence = alternatives[0].transcript
-
-                                if not sentence or len(sentence.strip()) == 0:
-                                    return
-
-                                is_final = getattr(message, 'is_final', True)
-                                confidence = getattr(alternatives[0], 'confidence', 1.0)
-
-                                # Create transcript segment
-                                segment = TranscriptSegment(
-                                    text=sentence,
-                                    is_final=is_final,
-                                    timestamp=datetime.now(),
-                                    confidence=confidence,
-                                )
-
-                                # Add to state buffer
-                                state.add_transcript_segment(segment)
-
-                                # Add to results
-                                self.results["transcripts"].append({
-                                    "text": sentence,
-                                    "is_final": is_final,
-                                    "confidence": confidence,
-                                    "timestamp": segment.timestamp.isoformat()
-                                })
-                                
-                                # Log to session logger
-                                if self.session_logger:
-                                    self.session_logger.log_transcript(
-                                        text=sentence,
-                                        is_final=is_final,
-                                        confidence=confidence,
-                                        timestamp=segment.timestamp
-                                    )
-
-                                # Write to JSON
-                                self._write_results()
-
-                                logger.info(f"[{'FINAL' if is_final else 'PARTIAL'}] {sentence}")
-
-                                # Topic tracking (Fast Loop) - same as WebSocket
-                                if is_final and state.should_update_topics():
-                                    finalized_text = state.consume_finalized_sentences()
-                                    asyncio.create_task(self._update_topics(finalized_text))
-
-                                # Fact checking (Slow Loop) - same as WebSocket
-                                if is_final:
-                                    asyncio.create_task(self._queue_fact_check(sentence))
-
-                    except Exception as e:
-                        logger.error(f"Error processing Deepgram message: {e}")
-
-                def on_error(error):
-                    """Handle Deepgram errors."""
-                    logger.error(f"Deepgram error: {error}")
-                    self.results["status"] = "error"
-                    self.results["error"] = str(error)
-                    self._write_results()
-
-                def on_close(_):
-                    """Handle Deepgram connection close."""
-                    logger.info("Deepgram connection closed")
-
-                def on_open(_):
-                    """Handle Deepgram connection open."""
-                    logger.info("Deepgram connection opened")
-
-                # Register event handlers
-                dg_connection.on(EventType.OPEN, on_open)
-                dg_connection.on(EventType.MESSAGE, on_message)
-                dg_connection.on(EventType.ERROR, on_error)
-                dg_connection.on(EventType.CLOSE, on_close)
-
-                # Start listening
-                listen_task = asyncio.create_task(dg_connection.start_listening())
-
-                # Stream audio file - adapted from test_wav_stream.py
-                with wave.open(str(self.audio_file), 'rb') as wav_file:
-                    frames_per_chunk = int((CHUNK_DURATION_MS / 1000.0) * wav_file.getframerate())
-                    total_frames = wav_file.getnframes()
-                    frames_sent = 0
-
-                    while frames_sent < total_frames and self.is_streaming:
-                        # Read chunk
-                        audio_chunk = wav_file.readframes(frames_per_chunk)
-
-                        if not audio_chunk:
-                            break
-
-                        # Convert stereo to mono (same as WebSocket endpoint)
-                        import struct
-                        samples = struct.unpack(f'<{len(audio_chunk)//2}h', audio_chunk)
-                        mono_samples = []
-                        for i in range(0, len(samples), 2):
-                            if i + 1 < len(samples):
-                                avg = (samples[i] + samples[i+1]) // 2
-                                mono_samples.append(avg)
-                            else:
-                                mono_samples.append(samples[i])
-
-                        mono_bytes = struct.pack(f'<{len(mono_samples)}h', *mono_samples)
-
-                        # Send to Deepgram
-                        await dg_connection.send_media(mono_bytes)
-
-                        # Update progress
-                        frames_sent += frames_per_chunk
-                        self.results["progress"] = (frames_sent / total_frames) * 100
-
-                        # Update every 5% progress
-                        if frames_sent % (total_frames // 20) < frames_per_chunk:
-                            self._write_results()
-                            logger.info(f"Progress: {self.results['progress']:.1f}%")
-
-                        # Sleep to simulate real-time streaming
-                        await asyncio.sleep(CHUNK_DURATION_MS / 1000.0)
-
-                # Stream complete
-                logger.info("Audio streaming complete, waiting for final transcriptions...")
-                await asyncio.sleep(5)  # Wait for final processing
-
-                # Clean up
-                listen_task.cancel()
-                try:
-                    await listen_task
-                except asyncio.CancelledError:
-                    pass
-
-            # Mark as complete
-            self.results["status"] = "complete"
-            self.results["completed_at"] = datetime.now().isoformat()
-            self.results["progress"] = 100.0
-            self._write_results()
-            
-            # Save session data to logs
-            self.save_session()
-
-            logger.info("Stream processing complete")
-
-        except asyncio.CancelledError:
-            logger.info("Stream cancelled")
-            self.results["status"] = "cancelled"
-            self._write_results()
-            # Save session even when cancelled
-            self.save_session()
-        except Exception as e:
-            logger.error(f"Stream processing error: {e}", exc_info=True)
-            self.results["status"] = "error"
-            self.results["error"] = str(e)
-            self._write_results()
-            # Save session even when error occurs
-            self.save_session()
-        finally:
-            self.is_streaming = False
-
-    async def _update_topics(self, text: str):
-        """Update topic tree (Fast Loop) - same as WebSocket endpoint."""
-        try:
-            topic_id = await topic_engine.update_topic_tree(text)
-
-            if topic_id:
-                summary = topic_engine.get_topic_summary()
-                current_timestamp = datetime.now()
-
-                # Add to results
-                self.results["topics"].append({
-                    "topic_id": topic_id,
-                    "topic": summary["current_topic"],
-                    "total_topics": summary["total_topics"],
-                    "timestamp": current_timestamp.isoformat()
-                })
-                
-                # Log to session logger
-                if self.session_logger:
-                    # Get topic node from state to extract keywords and sentence count
-                    topic_node = state.topic_tree.nodes.get(topic_id)
-                    if topic_node and 'data' in topic_node:
-                        node_data = topic_node['data']
-                        self.session_logger.log_topic(
-                            topic=summary["current_topic"],
-                            keywords=node_data.keywords if hasattr(node_data, 'keywords') else [],
-                            sentence_count=node_data.sentence_count if hasattr(node_data, 'sentence_count') else 0,
-                            timestamp=current_timestamp
-                        )
-
-                # Write to JSON
-                self._write_results()
-
-                logger.info(f"Topic update: {summary['current_topic']}")
-
-        except Exception as e:
-            logger.error(f"Error updating topics: {e}")
-
-    async def _queue_fact_check(self, sentence: str):
-        """Queue a fact check (Slow Loop) - same as WebSocket endpoint."""
-        try:
-            # Add to fact-checking queue
-            await state.fact_queue.put(sentence)
-
-            logger.info(f"Queued for fact check: {sentence[:50]}...")
-
-        except Exception as e:
-            logger.error(f"Error queuing fact check: {e}")
-
-    async def _monitor_fact_checks(self):
-        """
-        Monitor the fact_results list and capture new results.
-        This runs in the background and checks for new fact-check results
-        from the fact_engine.process_fact_queue() background task.
-        """
-        try:
-            while self.is_streaming:
-                # Check if there are new fact-check results
-                current_count = len(state.fact_results)
-
-                if current_count > self.last_fact_check_count:
-                    # Get new results
-                    new_results = state.fact_results[self.last_fact_check_count:current_count]
-
-                    for result in new_results:
-                        # Add to our results
-                        self.results["fact_checks"].append({
-                            "claim": result.claim,
-                            "verdict": result.verdict,
-                            "confidence": result.confidence,
-                            "explanation": result.explanation,
-                            "key_facts": result.key_facts,
-                            "sources": result.evidence_sources,
-                            "timestamp": result.timestamp.isoformat()
-                        })
-                        
-                        # Log to session logger
-                        if self.session_logger:
-                            self.session_logger.log_fact_check(
-                                claim=result.claim,
-                                verdict=result.verdict,
-                                confidence=result.confidence,
-                                explanation=result.explanation,
-                                key_facts=result.key_facts,
-                                evidence_sources=result.evidence_sources,
-                                timestamp=result.timestamp
-                            )
-
-                        logger.info(f"Fact check result: {result.verdict} - {result.claim[:50]}...")
-
-                    # Update counter
-                    self.last_fact_check_count = current_count
-
-                    # Write to JSON
-                    self._write_results()
-
-                # Check every second
-                await asyncio.sleep(1)
-
-        except asyncio.CancelledError:
-            logger.info("Fact check monitor stopped")
-        except Exception as e:
-            logger.error(f"Error in fact check monitor: {e}")
-
-
-# Global stream processor instance
-stream_processor = StreamProcessor()
diff --git a/backend/app/utils/__init__.py b/backend/app/utils/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/backend/app/utils/logger_util.py b/backend/app/utils/logger_util.py
deleted file mode 100644
index 3e0a24e..0000000
--- a/backend/app/utils/logger_util.py
+++ /dev/null
@@ -1,209 +0,0 @@
-"""
-JSON logging utility for debugging the Real-Time Podcast AI Assistant.
-Logs transcripts, topics, and fact-checks to separate JSON files in logs/ folder.
-Each session gets its own subdirectory.
-"""
-
-import json
-import logging
-from datetime import datetime
-from pathlib import Path
-from typing import Any, Dict, List, Optional
-
-
-logger = logging.getLogger(__name__)
-
-
-class DebugLogger:
-    """Handles JSON logging for debugging purposes."""
-
-    def __init__(self, base_logs_dir: Optional[Path] = None):
-        """
-        Initialize the debug logger with session info.
-        
-        Args:
-            base_logs_dir: Base directory for logs. If None, uses default location.
-        """
-        self.session_start = datetime.now()
-        self.session_id = self.session_start.strftime("%Y%m%d_%H%M%S")
-        self.transcripts: List[Dict[str, Any]] = []
-        self.topics: List[Dict[str, Any]] = []
-        self.facts: List[Dict[str, Any]] = []
-
-        # Create base logs directory if not provided
-        if base_logs_dir is None:
-            base_logs_dir = Path(__file__).parent / "logs"
-        
-        # Create session-specific directory
-        self.session_dir = base_logs_dir / f"session_{self.session_id}"
-        self.session_dir.mkdir(parents=True, exist_ok=True)
-        
-        # Define log file paths within session directory
-        self.transcript_log = self.session_dir / "transcripts.json"
-        self.topic_log = self.session_dir / "topics.json"
-        self.fact_log = self.session_dir / "facts.json"
-        self.session_log = self.session_dir / "session.json"
-
-        # Write session header
-        self._write_session_header()
-        logger.info(f"Debug logging initialized - Session: {self.session_id}")
-        logger.info(f"Session directory: {self.session_dir}")
-
-    def _write_session_header(self):
-        """Write session metadata."""
-        session_info = {
-            "session_id": self.session_id,
-            "start_time": self.session_start.isoformat(),
-            "session_directory": str(self.session_dir),
-            "logs": {
-                "transcripts": str(self.transcript_log),
-                "topics": str(self.topic_log),
-                "facts": str(self.fact_log),
-            }
-        }
-        with open(self.session_log, 'w') as f:
-            json.dump(session_info, f, indent=2)
-
-    def log_transcript(self, text: str, is_final: bool, confidence: float, timestamp: datetime = None):
-        """
-        Log a transcript segment.
-
-        Args:
-            text: Transcribed text
-            is_final: Whether this is a final transcript
-            confidence: Confidence score (0-1)
-            timestamp: Timestamp of the transcript
-        """
-        if timestamp is None:
-            timestamp = datetime.now()
-
-        entry = {
-            "timestamp": timestamp.isoformat(),
-            "text": text,
-            "is_final": is_final,
-            "confidence": confidence,
-            "session_time_seconds": (timestamp - self.session_start).total_seconds()
-        }
-
-        self.transcripts.append(entry)
-
-        # Append to file (one line per entry for easy streaming)
-        with open(self.transcript_log, 'a') as f:
-            f.write(json.dumps(entry) + "\n")
-
-        logger.debug(f"Logged transcript: {'FINAL' if is_final else 'PARTIAL'} - {text[:50]}...")
-
-    def log_topic(self, topic: str, keywords: List[str], sentence_count: int, timestamp: datetime = None):
-        """
-        Log a topic update.
-
-        Args:
-            topic: The detected topic
-            keywords: Keywords associated with the topic
-            sentence_count: Number of sentences accumulated for this topic
-            timestamp: Timestamp of topic detection
-        """
-        if timestamp is None:
-            timestamp = datetime.now()
-
-        entry = {
-            "timestamp": timestamp.isoformat(),
-            "topic": topic,
-            "keywords": keywords,
-            "sentence_count": sentence_count,
-            "session_time_seconds": (timestamp - self.session_start).total_seconds(),
-            "topic_number": len(self.topics) + 1
-        }
-
-        self.topics.append(entry)
-
-        # Append to file
-        with open(self.topic_log, 'a') as f:
-            f.write(json.dumps(entry) + "\n")
-
-        logger.info(f"Logged topic #{entry['topic_number']}: {topic} (keywords: {', '.join(keywords[:3])}...)")
-
-    def log_fact_check(
-        self,
-        claim: str,
-        verdict: str,
-        confidence: float,
-        explanation: str,
-        key_facts: List[str],
-        evidence_sources: List[str],
-        timestamp: datetime = None
-    ):
-        """
-        Log a fact-check result.
-
-        Args:
-            claim: The claim being checked
-            verdict: SUPPORTED, CONTRADICTED, or UNCERTAIN
-            confidence: Confidence score (0-1)
-            explanation: Explanation of the verdict
-            key_facts: Key facts found
-            evidence_sources: URLs of evidence sources
-            timestamp: Timestamp of fact check
-        """
-        if timestamp is None:
-            timestamp = datetime.now()
-
-        entry = {
-            "timestamp": timestamp.isoformat(),
-            "claim": claim,
-            "verdict": verdict,
-            "confidence": confidence,
-            "explanation": explanation,
-            "key_facts": key_facts,
-            "evidence_sources": evidence_sources,
-            "session_time_seconds": (timestamp - self.session_start).total_seconds(),
-            "fact_number": len(self.facts) + 1
-        }
-
-        self.facts.append(entry)
-
-        # Append to file
-        with open(self.fact_log, 'a') as f:
-            f.write(json.dumps(entry) + "\n")
-
-        logger.info(f"Logged fact #{entry['fact_number']}: {verdict} - {claim[:50]}...")
-
-    def get_summary(self) -> Dict[str, Any]:
-        """
-        Get a summary of the current session.
-
-        Returns:
-            Dictionary with session statistics
-        """
-        session_duration = (datetime.now() - self.session_start).total_seconds()
-
-        return {
-            "session_id": self.session_id,
-            "duration_seconds": session_duration,
-            "total_transcripts": len(self.transcripts),
-            "final_transcripts": sum(1 for t in self.transcripts if t["is_final"]),
-            "total_topics": len(self.topics),
-            "total_facts": len(self.facts),
-            "fact_verdicts": {
-                "SUPPORTED": sum(1 for f in self.facts if f["verdict"] == "SUPPORTED"),
-                "CONTRADICTED": sum(1 for f in self.facts if f["verdict"] == "CONTRADICTED"),
-                "UNCERTAIN": sum(1 for f in self.facts if f["verdict"] == "UNCERTAIN"),
-            }
-        }
-
-    def save_summary(self):
-        """Save final session summary."""
-        summary = self.get_summary()
-        summary["end_time"] = datetime.now().isoformat()
-
-        # Update session log with final summary
-        with open(self.session_log, 'r') as f:
-            session_data = json.load(f)
-
-        session_data["summary"] = summary
-
-        with open(self.session_log, 'w') as f:
-            json.dump(session_data, f, indent=2)
-
-        logger.info(f"Session summary saved: {summary['total_transcripts']} transcripts, "
-                   f"{summary['total_topics']} topics, {summary['total_facts']} facts")
diff --git a/backend/run.py b/backend/run.py
deleted file mode 100644
index 7aae6d3..0000000
--- a/backend/run.py
+++ /dev/null
@@ -1,22 +0,0 @@
-"""
-Backend entry point for Real-Time Podcast AI Assistant.
-Run this file to start the FastAPI server.
-"""
-
-import uvicorn
-import sys
-from pathlib import Path
-
-# Add the project root to the path to allow imports
-project_root = Path(__file__).parent.parent
-sys.path.insert(0, str(project_root))
-
-if __name__ == "__main__":
-    # Run the FastAPI app
-    uvicorn.run(
-        "backend.app.api.main:app",
-        host="0.0.0.0",
-        port=8000,
-        reload=True,
-        log_level="info"
-    )
diff --git a/backend/app/core/config.py b/config.py
similarity index 92%
rename from backend/app/core/config.py
rename to config.py
index 46d3ce3..6006a2e 100644
--- a/backend/app/core/config.py
+++ b/config.py
@@ -4,14 +4,9 @@ Handles environment variables, API keys, and LLM prompts.
 """
 
 import os
-from pathlib import Path
 from typing import Optional
 from pydantic_settings import BaseSettings, SettingsConfigDict
 
-# Get the backend directory path
-BACKEND_DIR = Path(__file__).parent.parent.parent
-ENV_FILE = BACKEND_DIR / ".env"
-
 
 class Settings(BaseSettings):
     """Application settings loaded from environment variables."""
@@ -24,6 +19,8 @@ class Settings(BaseSettings):
     # Configuration Parameters
     fact_check_rate_limit: int = 10  # seconds between fact checks
     topic_update_threshold: int = 5  # finalized sentences before topic update
+    claim_selection_batch_size: int = 10  # sentences to accumulate before selecting claims
+    max_claims_per_batch: int = 2  # max claims to select from each batch
 
     # Model Configuration
     together_model: str = "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
@@ -32,7 +29,7 @@ class Settings(BaseSettings):
     max_buffer_size: int = 1000  # Increased to hold more segments
 
     model_config = SettingsConfigDict(
-        env_file=str(ENV_FILE),
+        env_file=".env",
         env_file_encoding="utf-8",
         case_sensitive=False
     )
@@ -41,6 +38,10 @@ class Settings(BaseSettings):
 # Global settings instance
 settings = Settings()
 
+# Load claim selection prompt
+with open("CLAIM_SELECTION_PROMPT.txt", "r") as f:
+    CLAIM_SELECTION_PROMPT = f.read()
+
 
 # ============================================================================
 # LLM PROMPTS
diff --git a/docs/README.md b/docs/README.md
deleted file mode 100644
index c6cca9e..0000000
--- a/docs/README.md
+++ /dev/null
@@ -1,216 +0,0 @@
-# Real-Time Podcast AI Assistant
-
-An AI-powered assistant for live podcast transcription, topic tracking, and real-time fact-checking.
-
-## Features
-
-- **Live Transcription**: Real-time audio transcription using Deepgram Flux
-- **Topic Tracking**: Automatic semantic drift detection and conversation timeline
-- **Fact Checking**: 3-step verification pipeline (Detect â†’ Search â†’ Verify)
-- **Dual-Loop Architecture**: Fast loop for topics, slow loop for fact-checking
-- **WebSocket API**: Real-time communication with minimal latency
-
-## Architecture
-
-```
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚                     Client (Audio Stream)                    â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-                           â”‚ WebSocket
-                           â–¼
-â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
-â”‚                       FastAPI Server                         â”‚
-â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
-â”‚  â”‚              Deepgram (Transcription)                   â”‚ â”‚
-â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
-â”‚                 â”‚                                             â”‚
-â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
-â”‚     â”‚   FAST LOOP          â”‚       â”‚   SLOW LOOP         â”‚   â”‚
-â”‚     â”‚   Topic Tracker      â”‚       â”‚   Fact Checker      â”‚   â”‚
-â”‚     â”‚   (NetworkX Graph)   â”‚       â”‚   (3-Step Pipeline) â”‚   â”‚
-â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
-â”‚                                                               â”‚
-â”‚              State Manager (Central State)                   â”‚
-â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-```
-
-## Installation
-
-1. **Clone the repository**:
-   ```bash
-   cd lauzhack-2025
-   ```
-
-2. **Install dependencies**:
-   ```bash
-   pip install -r requirements.txt
-   ```
-
-3. **Set up environment variables**:
-   ```bash
-   cp .env.example .env
-   ```
-
-   Edit `.env` and add your API keys:
-   ```
-   DEEPGRAM_API_KEY=your_deepgram_key
-   TOGETHER_API_KEY=your_together_ai_key
-   HUGGINGFACE_API_KEY=your_hf_key  # Optional
-   ```
-
-4. **Verify setup** (optional but recommended):
-   ```bash
-   python verify_setup.py
-   ```
-
-## Usage
-
-### Start the Server
-
-```bash
-python main.py
-```
-
-The server will start on `http://localhost:8000`
-
-### API Endpoints
-
-- `GET /` - Health check
-- `GET /stats` - System statistics
-- `GET /topics` - Topic timeline
-- `GET /facts` - Recent fact-check results
-- `WebSocket /listen` - Main audio streaming endpoint
-- `WebSocket /facts/stream` - Real-time fact results stream
-
-### WebSocket Client Example
-
-```python
-import asyncio
-import websockets
-import pyaudio
-
-async def stream_audio():
-    uri = "ws://localhost:8000/listen"
-
-    async with websockets.connect(uri) as websocket:
-        # Configure audio
-        audio = pyaudio.PyAudio()
-        stream = audio.open(
-            format=pyaudio.paInt16,
-            channels=1,
-            rate=16000,
-            input=True,
-            frames_per_buffer=8000
-        )
-
-        # Stream audio
-        while True:
-            data = stream.read(8000)
-            await websocket.send(data)
-
-            # Receive transcription
-            response = await websocket.recv()
-            print(response)
-
-asyncio.run(stream_audio())
-```
-
-## Configuration
-
-Edit `config.py` or set environment variables:
-
-- `FACT_CHECK_RATE_LIMIT`: Seconds between fact checks (default: 10)
-- `TOPIC_UPDATE_THRESHOLD`: Sentences before topic update (default: 5)
-- `TOGETHER_MODEL`: LLM model to use (default: Llama-3.1-70B)
-
-## Project Structure
-
-```
-lauzhack-2025/
-â”œâ”€â”€ main.py              # FastAPI server & WebSocket endpoints
-â”œâ”€â”€ config.py            # Configuration & prompts
-â”œâ”€â”€ state_manager.py     # Central state management
-â”œâ”€â”€ topic_engine.py      # Topic tracking & semantic drift
-â”œâ”€â”€ fact_engine.py       # 3-step fact-checking pipeline
-â”œâ”€â”€ requirements.txt     # Python dependencies
-â”œâ”€â”€ .env.example         # Environment template
-â””â”€â”€ README.md           # This file
-```
-
-## How It Works
-
-### Fast Loop (Topic Tracking)
-
-1. Partial transcripts arrive continuously
-2. Every 5 finalized sentences â†’ extract topic using LLM
-3. Compare with current topic using embeddings
-4. If semantic drift detected â†’ create new topic node
-5. Update NetworkX graph with topic timeline
-
-### Slow Loop (Fact Checking)
-
-1. **Detect**: LLM determines if sentence contains factual claim
-2. **Search**: DuckDuckGo finds evidence (with source links)
-3. **Verify**: LLM compares claim vs evidence
-4. Rate-limited to 1 check per 10 seconds (configurable)
-5. Results stored and streamed to clients
-
-## Development
-
-### Testing
-
-```bash
-# Run the server in development mode
-python main.py
-
-# In another terminal, test the API
-curl http://localhost:8000/stats
-```
-
-### TODO Improvements
-
-- [ ] Replace mock embeddings with sentence-transformers
-- [ ] Add speaker diarization
-- [ ] Implement conversation export
-- [ ] Add authentication
-- [ ] Create web UI
-- [ ] Add database persistence
-- [ ] Implement caching for LLM responses
-
-## Tech Stack
-
-- **Framework**: FastAPI 0.115.0
-- **Audio**: Deepgram SDK 5.3.0
-- **LLM**: Together.ai 1.3.4 (Llama-3.1-70B)
-- **Search**: DuckDuckGo Search 8.1.1
-- **Graph**: NetworkX 3.4.2
-- **Async**: Python asyncio
-- **Python**: 3.8+ required
-
-## License
-
-MIT License - Built for LauzHack 2025
-
-## Troubleshooting
-
-**Issue**: Deepgram connection fails
-- Check API key in `.env`
-- Ensure audio format is PCM 16kHz mono
-
-**Issue**: Rate limiting on search
-- Increase `FACT_CHECK_RATE_LIMIT` in `.env`
-- Reduce `SEARCH_CONFIG.max_results` in `config.py`
-
-**Issue**: LLM responses fail to parse
-- Check Together.ai API key
-- Verify model availability
-- Check logs for JSON parsing errors
-
-## Contributing
-
-This is a hackathon MVP. Contributions welcome!
-
-1. Fork the repository
-2. Create a feature branch
-3. Make your changes
-4. Submit a pull request
diff --git a/backend/app/engines/fact_engine.py b/fact_engine.py
similarity index 67%
rename from backend/app/engines/fact_engine.py
rename to fact_engine.py
index b0dea7b..a715648 100644
--- a/backend/app/engines/fact_engine.py
+++ b/fact_engine.py
@@ -14,13 +14,14 @@ except ImportError:
     from duckduckgo_search import DDGS  # Fallback for older package name
 from together import Together
 
-from backend.app.core.config import (
+from config import (
     settings,
     CLAIM_DETECTION_PROMPT,
     CLAIM_VERIFICATION_PROMPT,
+    CLAIM_SELECTION_PROMPT,
     SEARCH_CONFIG,
 )
-from backend.app.core.state_manager import state, FactCheckResult
+from state_manager import state, FactCheckResult
 
 logger = logging.getLogger(__name__)
 
@@ -35,6 +36,132 @@ class FactEngine:
         self.client = Together(api_key=settings.together_api_key)
         self.search_client = DDGS()
 
+    async def _generate_search_query(self, claim: str) -> str:
+        """
+        Generate an optimized search query from a claim.
+        
+        Extracts key facts, entities, and numbers to create a better search query.
+        
+        Args:
+            claim: The claim to generate a search query for
+            
+        Returns:
+            Optimized search query string
+        """
+        try:
+            prompt = f"""Convert this claim into an optimized web search query.
+
+Claim: {claim}
+
+Instructions:
+1. Extract the CORE FACTUAL ASSERTION (remove filler, opinions, context)
+2. Identify KEY ENTITIES (names, organizations, places, numbers, dates)
+3. Create a concise search query (3-8 words) that will find relevant evidence
+
+Examples:
+- Claim: "eighty percent not maybe ninety percent of the funding goes to the democrats"
+  Query: "political funding distribution democrats republicans percentage"
+  
+- Claim: "ninety percent of the money is going to your opponents"
+  Query: "campaign finance political party funding distribution"
+
+Output ONLY the search query, nothing else."""
+
+            loop = asyncio.get_event_loop()
+            response = await loop.run_in_executor(
+                None,
+                lambda: self.client.chat.completions.create(
+                    model=settings.together_model,
+                    messages=[
+                        {"role": "system", "content": "You are a search query optimization assistant."},
+                        {"role": "user", "content": prompt},
+                    ],
+                    temperature=0.1,
+                    max_tokens=50,
+                ),
+            )
+            
+            search_query = response.choices[0].message.content.strip()
+            # Remove quotes if present
+            search_query = search_query.strip('"\'')
+            
+            return search_query
+            
+        except Exception as e:
+            logger.error(f"Search query generation failed: {e}")
+            # Fallback: use claim as-is
+            return claim
+
+    async def select_claims(self, statements: List[str]) -> List[str]:
+        """
+        Select the most important factual claims from a batch of statements.
+        
+        This replaces individual claim detection for each sentence with a single
+        batched selection that uses context to identify the most relevant claims.
+
+        Args:
+            statements: List of recent statements to analyze
+
+        Returns:
+            List of selected claims (extracted with full context)
+        """
+        try:
+            # Concatenate statements into a paragraph for better context
+            full_text = " ".join(statements)
+            
+            prompt = CLAIM_SELECTION_PROMPT.format(
+                text=full_text,
+                max_claims=settings.max_claims_per_batch
+            )
+
+            # Run LLM call in thread pool
+            loop = asyncio.get_event_loop()
+            try:
+                response = await loop.run_in_executor(
+                    None,
+                    lambda: self.client.chat.completions.create(
+                        model=settings.together_model,
+                        messages=[
+                            {
+                                "role": "system",
+                                "content": "You are a claim selection assistant. Always respond in valid JSON format.",
+                            },
+                            {"role": "user", "content": prompt},
+                        ],
+                        temperature=0.2,
+                        max_tokens=400,
+                    ),
+                )
+            except Exception as api_error:
+                print(f"âŒ API call failed: {api_error}")
+                return []
+
+            content = response.choices[0].message.content.strip()
+
+            # Parse JSON response
+            if content.startswith("```"):
+                content = content.split("```")[1]
+                if content.startswith("json"):
+                    content = content[4:]
+                content = content.strip()
+
+            result = json.loads(content)
+            
+            selected = [claim["claim"] for claim in result.get("selected_claims", [])]
+            
+            # Print selected claims to terminal
+            if selected:
+                print(f"âœ… SELECTED CLAIMS:")
+                for claim in selected:
+                    print(f"   â€¢ {claim}")
+                print()
+            
+            return selected
+
+        except Exception as e:
+            logger.error(f"Claim selection failed: {e}")
+            return []
+
     async def detect_claim(self, statement: str) -> Optional[Dict]:
         """
         Step 1: Detect if the statement contains a factual claim.
@@ -109,6 +236,10 @@ class FactEngine:
 
         try:
             logger.info(f"Searching evidence for: {claim[:100]}...")
+            
+            # Generate a better search query from the claim
+            search_query = await self._generate_search_query(claim)
+            print(f"ğŸ” Search query: {search_query}")
 
             # Run search in thread pool to avoid blocking
             loop = asyncio.get_event_loop()
@@ -116,7 +247,7 @@ class FactEngine:
                 None,
                 lambda: list(
                     self.search_client.text(
-                        claim,
+                        search_query,
                         region=SEARCH_CONFIG["region"],
                         safesearch=SEARCH_CONFIG["safesearch"],
                         max_results=max_results,
@@ -312,8 +443,15 @@ class FactEngine:
                     state.add_fact_result(result)
                     state.mark_fact_check_performed()
                     logger.info(f"Fact check stored: {result.verdict}")
-
-                    # Note: Logging is handled by stream_processor when monitoring fact results
+                    
+                    # Print result to terminal
+                    print(f"\n{'='*60}")
+                    print(f"ğŸ“Š FACT CHECK RESULT")
+                    print(f"{'='*60}")
+                    print(f"Claim: {result.claim}")
+                    print(f"Verdict: {result.verdict} (Confidence: {result.confidence:.0%})")
+                    print(f"Explanation: {result.explanation}")
+                    print(f"{'='*60}\n")
 
                 # Mark task as done
                 state.fact_queue.task_done()
diff --git a/frontend/README.md b/frontend/README.md
deleted file mode 100644
index a0b889a..0000000
--- a/frontend/README.md
+++ /dev/null
@@ -1,32 +0,0 @@
-# Frontend
-
-This directory will contain the frontend implementation for the Real-Time Podcast AI Assistant.
-
-## Structure
-
-```
-frontend/
-â”œâ”€â”€ public/          # Static assets
-â”œâ”€â”€ src/
-â”‚   â”œâ”€â”€ components/  # React components
-â”‚   â”œâ”€â”€ pages/       # Page components
-â”‚   â”œâ”€â”€ styles/      # CSS/styling files
-â”‚   â”œâ”€â”€ utils/       # Utility functions
-â”‚   â””â”€â”€ assets/      # Images, fonts, etc.
-â””â”€â”€ README.md
-```
-
-## Getting Started
-
-The frontend will be implemented based on Figma designs. This structure is ready for:
-- React/Next.js implementation
-- Component-based architecture
-- Integration with the backend WebSocket API
-
-## TODO
-- [ ] Set up React/Next.js project
-- [ ] Implement Figma designs
-- [ ] Connect to backend WebSocket endpoint
-- [ ] Add real-time transcription display
-- [ ] Add topic visualization
-- [ ] Add fact-checking results display
diff --git a/frontend/package.json b/frontend/package.json
deleted file mode 100644
index 4612ec3..0000000
--- a/frontend/package.json
+++ /dev/null
@@ -1,13 +0,0 @@
-{
-  "name": "podcast-ai-assistant-frontend",
-  "version": "0.1.0",
-  "private": true,
-  "description": "Frontend for Real-Time Podcast AI Assistant",
-  "scripts": {
-    "dev": "echo 'Frontend not yet implemented. See README.md'",
-    "build": "echo 'Frontend not yet implemented. See README.md'",
-    "start": "echo 'Frontend not yet implemented. See README.md'"
-  },
-  "dependencies": {},
-  "devDependencies": {}
-}
diff --git a/backend/app/api/main.py b/main.py
similarity index 66%
rename from backend/app/api/main.py
rename to main.py
index fd3b8dd..22de517 100644
--- a/backend/app/api/main.py
+++ b/main.py
@@ -8,24 +8,24 @@ import logging
 from contextlib import asynccontextmanager
 from datetime import datetime
 from typing import AsyncGenerator
-from deepgram import AsyncDeepgramClient
-from fastapi import FastAPI, WebSocket, WebSocketDisconnect, UploadFile, File, HTTPException
+
+from fastapi import FastAPI, WebSocket, WebSocketDisconnect
 from fastapi.responses import JSONResponse
-from deepgram import DeepgramClient
+from deepgram import AsyncDeepgramClient
 from deepgram.core.events import EventType
 
-from backend.app.core.config import settings
-from backend.app.core.state_manager import state, TranscriptSegment
-from backend.app.engines.topic_engine import topic_engine
-from backend.app.engines.fact_engine import fact_engine
-from backend.app.services.stream_processor import stream_processor
+from config import settings
+from state_manager import state, TranscriptSegment
+from topic_engine import topic_engine
+from fact_engine import fact_engine
 
-# Configure logging
+# Disable terminal logging
 logging.basicConfig(
-    level=logging.DEBUG,  # Changed to DEBUG to see detailed messages
+    level=logging.CRITICAL,  # Only critical errors
     format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
 )
 logger = logging.getLogger(__name__)
+logger.disabled = True
 
 
 # Background task for fact-checking queue
@@ -52,6 +52,16 @@ async def lifespan(app: FastAPI) -> AsyncGenerator:
 
     # Shutdown
     logger.info("Shutting down...")
+    
+    # Save topic tree before shutting down
+    if len(state.topic_tree.nodes) > 0:
+        from pathlib import Path
+        logs_dir = Path("logs")
+        logs_dir.mkdir(exist_ok=True)
+        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+        filepath = logs_dir / f"topic_tree_{timestamp}.json"
+        state.export_topic_tree_json(str(filepath))
+    
     # if fact_queue_task:
     #     fact_queue_task.cancel()
     #     try:
@@ -124,191 +134,6 @@ async def get_transcript():
     })
 
 
-@app.post("/process-audio")
-async def process_audio(file: UploadFile = File(...)):
-    """
-    Process an audio file and return transcription, topics, and fact-checks.
-
-    This endpoint is designed for demos - it processes a complete audio file
-    and returns all analysis results in one response.
-
-    Args:
-        file: Audio file (WAV, MP3, etc.)
-
-    Returns:
-        JSON with transcript, topics, and fact-check results
-    """
-    logger.info(f"Processing audio file: {file.filename}")
-
-    try:
-        # Read the audio file
-        audio_data = await file.read()
-
-        # Initialize Deepgram client
-        deepgram = DeepgramClient(api_key=settings.deepgram_api_key)
-
-        logger.info("Sending audio to Deepgram for transcription...")
-
-        # Send to Deepgram prerecorded API using the correct method
-        response = deepgram.listen.v1.media.transcribe_file(
-            request=audio_data,
-            model="nova-3",
-            smart_format=True,
-            punctuate=True,
-            paragraphs=True,
-            utterances=True,
-        )
-
-        # Extract transcript
-        transcript_data = response.results
-        if not transcript_data or not transcript_data.channels:
-            raise HTTPException(status_code=500, detail="No transcription results from Deepgram")
-
-        # Get the full transcript
-        alternatives = transcript_data.channels[0].alternatives
-        if not alternatives:
-            raise HTTPException(status_code=500, detail="No alternatives in transcription")
-
-        full_transcript = alternatives[0].transcript
-        paragraphs = alternatives[0].paragraphs.paragraphs if hasattr(alternatives[0], 'paragraphs') else []
-
-        logger.info(f"Transcription complete: {len(full_transcript)} characters")
-
-        # Process topics from paragraphs
-        topics_list = []
-        if paragraphs:
-            for i, para in enumerate(paragraphs[:5]):  # Process first 5 paragraphs for demo
-                para_text = " ".join([sentence.text for sentence in para.sentences])
-                if len(para_text) > 50:  # Only process substantial paragraphs
-                    topic_result = await topic_engine.extract_topic(para_text)
-                    if topic_result:
-                        topic, keywords = topic_result
-                        topics_list.append({
-                            "topic": topic,
-                            "keywords": keywords,
-                            "text_sample": para_text[:100] + "..."
-                        })
-
-        logger.info(f"Extracted {len(topics_list)} topics")
-
-        # Process fact-checking on key sentences
-        fact_checks = []
-        if paragraphs:
-            sentences_to_check = []
-            for para in paragraphs[:3]:  # Check first 3 paragraphs
-                for sentence in para.sentences[:2]:  # First 2 sentences per paragraph
-                    if len(sentence.text) > 30:  # Only substantial sentences
-                        sentences_to_check.append(sentence.text)
-
-            # Fact-check each sentence
-            for sentence in sentences_to_check[:5]:  # Limit to 5 for demo
-                logger.info(f"Fact-checking: {sentence[:50]}...")
-
-                # Run the full fact-checking pipeline
-                result = await fact_engine.check_fact(sentence)
-                if result:
-                    fact_checks.append({
-                        "claim": result.claim,
-                        "verdict": result.verdict,
-                        "confidence": result.confidence,
-                        "explanation": result.explanation,
-                        "key_facts": result.key_facts,
-                        "sources": result.evidence_sources
-                    })
-
-        logger.info(f"Completed {len(fact_checks)} fact-checks")
-
-        # Return comprehensive results
-        return JSONResponse(content={
-            "status": "success",
-            "filename": file.filename,
-            "transcript": {
-                "full_text": full_transcript,
-                "word_count": len(full_transcript.split()),
-                "paragraph_count": len(paragraphs) if paragraphs else 0
-            },
-            "topics": topics_list,
-            "fact_checks": fact_checks,
-            "summary": {
-                "total_topics": len(topics_list),
-                "total_fact_checks": len(fact_checks),
-                "verified_claims": len([f for f in fact_checks if f["verdict"] == "SUPPORTED"]),
-                "false_claims": len([f for f in fact_checks if f["verdict"] == "REFUTED"])
-            }
-        })
-
-    except Exception as e:
-        logger.error(f"Error processing audio: {e}", exc_info=True)
-        raise HTTPException(status_code=500, detail=f"Error processing audio: {str(e)}")
-
-
-# ============================================================================
-# STREAMING API ENDPOINTS
-# Server-side streaming for MVP demo
-# ============================================================================
-
-@app.post("/api/stream/start")
-async def start_stream():
-    """
-    Start server-side streaming of the hardcoded audio file.
-
-    This endpoint:
-    1. Starts processing the audio file in chunks (simulating real-time)
-    2. Runs transcription, topic detection, and fact-checking
-    3. Writes results incrementally to stream_output.json
-
-    Returns:
-        Status and metadata about the stream
-    """
-    result = await stream_processor.start_stream()
-
-    if "error" in result:
-        raise HTTPException(status_code=400, detail=result["error"])
-
-    return result
-
-
-@app.post("/api/stream/stop")
-async def stop_stream():
-    """
-    Stop the current streaming session.
-
-    Returns:
-        Confirmation of stream stop
-    """
-    result = await stream_processor.stop_stream()
-
-    if "error" in result:
-        raise HTTPException(status_code=400, detail=result["error"])
-
-    return result
-
-
-@app.get("/api/stream/status")
-async def get_stream_status():
-    """
-    Get the current status of the stream.
-
-    Returns:
-        Current streaming status, progress, and counts
-    """
-    return stream_processor.get_status()
-
-
-@app.get("/api/stream/results")
-async def get_stream_results():
-    """
-    Get the complete current results of the stream.
-
-    This returns the same data that's written to stream_output.json,
-    providing an API alternative to reading the file directly.
-
-    Returns:
-        Complete results including transcripts, topics, and fact-checks
-    """
-    return stream_processor.get_results()
-
-
 @app.websocket("/listen")
 async def websocket_endpoint(websocket: WebSocket):
     """
@@ -400,9 +225,9 @@ async def websocket_endpoint(websocket: WebSocket):
                                 finalized_text = state.consume_finalized_sentences()
                                 asyncio.create_task(update_topics_async(websocket, finalized_text))
 
-                            # Fact checking (Slow Loop - queued)
+                            # Fact checking (Slow Loop - batched selection)
                             if is_final:
-                                asyncio.create_task(queue_fact_check_async(websocket, sentence))
+                                asyncio.create_task(batch_claim_selection_async(websocket, sentence))
 
                 except Exception as e:
                     logger.error(f"Error processing Deepgram message: {e}")
@@ -511,23 +336,78 @@ async def update_topics_async(websocket: WebSocket, text: str):
         topic_id = await topic_engine.update_topic_tree(text)
 
         if topic_id:
-            summary = topic_engine.get_topic_summary()
+            # Get topic data
+            topic_node = state.topic_tree.nodes[topic_id]["data"]
+            
+            # Try to get image URL from topic_images (may not be available yet)
+            image_url = None
+            for img_entry in reversed(state.topic_images):
+                if img_entry["topic_id"] == topic_id:
+                    image_url = img_entry["image_url"]
+                    break
+            
             await websocket.send_json(
                 {
                     "type": "topic_update",
                     "topic_id": topic_id,
-                    "current_topic": summary["current_topic"],
-                    "total_topics": summary["total_topics"],
+                    "current_topic": topic_node.topic,
+                    "image_url": image_url,
+                    "total_topics": len(state.topic_tree.nodes),
                 }
             )
-            logger.info(f"Topic update sent: {summary['current_topic']}")
+            logger.info(f"Topic update sent: {topic_node.topic}")
 
     except Exception as e:
         logger.error(f"Error updating topics: {e}")
 
 
+async def batch_claim_selection_async(websocket: WebSocket, sentence: str):
+    """
+    Accumulate sentences and periodically select the most important claims to fact-check.
+    
+    This replaces the old approach of queuing every sentence individually.
+    
+    Args:
+        websocket: WebSocket to send updates to
+        sentence: Finalized sentence to add to the batch
+    """
+    try:
+        # Add sentence to batch
+        state.sentence_batch.append(sentence)
+        
+        # Check if batch is full
+        if len(state.sentence_batch) >= settings.claim_selection_batch_size:
+            # Print batch being processed
+            batch_text = " ".join(state.sentence_batch)
+            print(f"\nğŸ“‹ BATCH: {batch_text}\n")
+            
+            # Select important claims from the batch
+            selected_claims = await fact_engine.select_claims(state.sentence_batch)
+            
+            # Queue selected claims for fact-checking
+            for claim in selected_claims:
+                await state.fact_queue.put(claim)
+                
+                # Notify client
+                await websocket.send_json(
+                    {
+                        "type": "claim_selected",
+                        "claim": claim,
+                        "queue_size": state.fact_queue.qsize(),
+                    }
+                )
+            
+            # Clear the batch
+            state.sentence_batch.clear()
+            
+    except Exception as e:
+        logger.error(f"Error in batch claim selection: {e}")
+
+
 async def queue_fact_check_async(websocket: WebSocket, sentence: str):
     """
+    OLD IMPLEMENTATION - kept for reference but not used.
+    
     Asynchronously queue a fact check (Slow Loop).
 
     This function does NOT block - it just queues the sentence
diff --git a/backend/requirements.txt b/requirements.txt
similarity index 100%
rename from backend/requirements.txt
rename to requirements.txt
diff --git a/backend/app/core/state_manager.py b/state_manager.py
similarity index 71%
rename from backend/app/core/state_manager.py
rename to state_manager.py
index 4019e48..75b0632 100644
--- a/backend/app/core/state_manager.py
+++ b/state_manager.py
@@ -10,7 +10,7 @@ from datetime import datetime
 from typing import Deque, Dict, List, Optional
 import networkx as nx
 
-from backend.app.core.config import settings
+from config import settings
 
 
 @dataclass
@@ -83,10 +83,15 @@ class StateManager:
         self.topic_tree: nx.DiGraph = nx.DiGraph()
         self.current_topic_id: Optional[str] = None
         self.topic_counter: int = 0
+        self.topic_path: List[str] = []  # Sequential history of topics visited
+        self.topic_images: List[Dict[str, Optional[str]]] = []  # Sequential list of {topic_id, topic, image_url}
 
         # Fact Check Queue - async queue for fact-checking tasks
         self.fact_queue: asyncio.Queue = asyncio.Queue()
 
+        # Sentence accumulation for batched claim selection
+        self.sentence_batch: List[str] = []
+
         # Fact Check Results - stored results for retrieval
         self.fact_results: List[FactCheckResult] = []
 
@@ -182,6 +187,8 @@ class StateManager:
     ) -> str:
         """
         Add a new topic node to the topic tree.
+        
+        Always links to the current topic, creating a temporal chain.
 
         Args:
             topic: Topic name
@@ -196,15 +203,18 @@ class StateManager:
         self.stats["topics_identified"] += 1
 
         node = TopicNode(
-            topic=topic, keywords=keywords, timestamp=timestamp, sentence_count=1
+            topic=topic, keywords=keywords, timestamp=timestamp, 
+            sentence_count=1
         )
 
         self.topic_tree.add_node(topic_id, data=node)
 
-        # Link to previous topic if exists
+        # Link to current topic (temporal chain)
         if self.current_topic_id is not None:
             self.topic_tree.add_edge(self.current_topic_id, topic_id)
-
+        
+        # Add to topic path history
+        self.topic_path.append(topic_id)
         self.current_topic_id = topic_id
         return topic_id
 
@@ -213,6 +223,35 @@ class StateManager:
         if self.current_topic_id is not None:
             node_data = self.topic_tree.nodes[self.current_topic_id]["data"]
             node_data.sentence_count += 1
+    
+    def switch_to_topic(self, topic_id: str) -> None:
+        """
+        Switch to an existing topic (when conversation returns to it).
+        
+        Args:
+            topic_id: Topic ID to switch to
+        """
+        if topic_id in self.topic_tree.nodes:
+            self.current_topic_id = topic_id
+            self.topic_path.append(topic_id)
+            # Increment sentence count when returning to topic
+            node_data = self.topic_tree.nodes[topic_id]["data"]
+            node_data.sentence_count += 1
+    
+    def add_topic_image(self, topic_id: str, topic: str, image_url: Optional[str]) -> None:
+        """
+        Record an image URL for a topic (called after async image search completes).
+        
+        Args:
+            topic_id: Topic ID
+            topic: Topic name
+            image_url: URL of the image (or None if search failed)
+        """
+        self.topic_images.append({
+            "topic_id": topic_id,
+            "topic": topic,
+            "image_url": image_url
+        })
 
     def get_topic_timeline(self) -> List[TopicNode]:
         """
@@ -232,6 +271,53 @@ class StateManager:
         # Sort by timestamp
         nodes.sort(key=lambda x: x.timestamp)
         return nodes
+    
+    def export_topic_tree_json(self, filepath: str) -> None:
+        """
+        Export the topic tree to a JSON file for visualization.
+        
+        Args:
+            filepath: Path to save JSON file
+        """
+        import json
+        
+        # Build tree structure
+        tree_data = {
+            "nodes": [],
+            "edges": [],
+            "topic_path": self.topic_path,
+            "topic_images": self.topic_images,  # Sequential list of topic images
+            "metadata": {
+                "total_topics": len(self.topic_tree.nodes),
+                "current_topic": self.current_topic_id,
+                "generated_at": datetime.now().isoformat()
+            }
+        }
+        
+        # Add nodes
+        for node_id in self.topic_tree.nodes:
+            node_data = self.topic_tree.nodes[node_id]["data"]
+            tree_data["nodes"].append({
+                "id": node_id,
+                "topic": node_data.topic,
+                "keywords": node_data.keywords,
+                "timestamp": node_data.timestamp.isoformat(),
+                "sentence_count": node_data.sentence_count,
+                "weight": node_data.weight
+            })
+        
+        # Add edges (parent-child relationships)
+        for source, target in self.topic_tree.edges:
+            tree_data["edges"].append({
+                "source": source,
+                "target": target
+            })
+        
+        # Write to file
+        with open(filepath, 'w') as f:
+            json.dump(tree_data, f, indent=2)
+        
+        print(f"\nğŸ’¾ Topic tree saved to: {filepath}")
 
     def get_stats(self) -> Dict:
         """
diff --git a/backend/tests/test_audio/README.md b/test_audio/README.md
similarity index 100%
rename from backend/tests/test_audio/README.md
rename to test_audio/README.md
diff --git a/test_audio_client.py b/test_audio_client.py
new file mode 100644
index 0000000..705824a
--- /dev/null
+++ b/test_audio_client.py
@@ -0,0 +1,250 @@
+"""
+Audio-based test client for Real-Time Podcast AI Assistant.
+Uses FFmpeg to convert audio files or streams to linear16 PCM format.
+"""
+
+import asyncio
+import json
+import subprocess
+import websockets
+from pathlib import Path
+
+
+# FFmpeg path - UPDATE THIS to point to your ffmpeg.exe
+# Download from: https://www.gyan.dev/ffmpeg/builds/ffmpeg-release-essentials.zip
+# Extract and point to ffmpeg.exe location
+FFMPEG_PATH = r"C:\ffmpeg\bin\ffmpeg.exe"  # Update this path!
+
+# Alternative: If you have ffmpeg in PATH, you can use just "ffmpeg"
+# FFMPEG_PATH = "ffmpeg"
+
+# Test audio source options:
+
+# Option 1: Local audio file (recommended for demo)
+# Place your podcast file (MP3, WAV, etc.) in the 'test_audio' folder
+# AUDIO_SOURCE = r"C:\Users\loizi\PycharmProjects\lauzhack-2025\test_audio\podcast_sample.wav"
+
+# Option 2: Use MP3 instead
+# AUDIO_SOURCE = r"C:\Users\loizi\PycharmProjects\lauzhack-2025\test_audio\podcast_sample.mp3"
+
+# Option 3: BBC World Service live stream (for real-time testing)
+AUDIO_SOURCE = "http://stream.live.vc.bbcmedia.co.uk/bbc_world_service"
+
+# Option 4: Any other local audio file (MP3, WAV, M4A, AAC, FLAC, OGG, etc.)
+# AUDIO_SOURCE = r"C:\path\to\your\audio.wav"
+
+
+async def stream_audio_to_deepgram():
+    """
+    Stream audio from a source (URL or file) to the server.
+    FFmpeg converts to linear16 PCM format that Deepgram expects.
+    """
+    uri = "ws://localhost:8000/listen"
+
+    # Check if source is a file
+    is_file = not AUDIO_SOURCE.startswith("http://") and not AUDIO_SOURCE.startswith("https://")
+
+    if is_file:
+        source_path = Path(AUDIO_SOURCE)
+        if not source_path.exists():
+            print(f"âŒ Audio file not found: {AUDIO_SOURCE}")
+            print(f"\nPlease:")
+            print(f"1. Create folder: test_audio")
+            print(f"2. Place your MP3 file there as: podcast_sample.mp3")
+            print(f"   OR update AUDIO_SOURCE in the script")
+            return
+
+        print(f"ğŸ“ Audio file: {source_path.name}")
+        print(f"   Size: {source_path.stat().st_size / 1024 / 1024:.2f} MB")
+    else:
+        print(f"ğŸŒ Streaming from: {AUDIO_SOURCE}")
+
+    print(f"\nConnecting to {uri}...")
+
+    try:
+        async with websockets.connect(uri) as websocket:
+            print("âœ“ Connected to server")
+
+            # FFmpeg command to convert audio to linear16 PCM
+            ffmpeg_cmd = [
+                FFMPEG_PATH,
+                '-i', AUDIO_SOURCE,          # Input source
+                '-f', 's16le',               # Output format: 16-bit little-endian PCM
+                '-ar', '16000',              # Sample rate: 16kHz
+                '-ac', '1',                  # Channels: mono
+                '-loglevel', 'error',        # Only show errors
+                '-'                          # Output to stdout
+            ]
+
+            print("âœ“ Starting FFmpeg audio conversion...")
+
+            # Start FFmpeg process
+            process = await asyncio.create_subprocess_exec(
+                *ffmpeg_cmd,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE
+            )
+
+            print("âœ“ Streaming audio data to server...")
+            print("=" * 50)
+            print("Listening for transcriptions (Press Ctrl+C to stop)...")
+            print("=" * 50)
+
+            # Create tasks for sending and receiving
+            async def send_audio():
+                """Send audio chunks to the server."""
+                try:
+                    # Read and send audio in chunks (~80ms recommended)
+                    # 2560 bytes = ~80ms at 16kHz linear16
+                    chunk_size = 2560
+
+                    while True:
+                        chunk = await process.stdout.read(chunk_size)
+                        if not chunk:
+                            print("\nâœ“ Audio stream ended")
+                            break
+
+                        # Send binary audio data
+                        await websocket.send(chunk)
+
+                except Exception as e:
+                    print(f"\nâœ— Error sending audio: {e}")
+
+            async def receive_transcriptions():
+                """Receive and display transcriptions from the server."""
+                try:
+                    while True:
+                        response = await websocket.recv()
+                        data = json.loads(response)
+
+                        if data.get("type") == "transcript":
+                            is_final = data.get("is_final", False)
+                            text = data.get("text", "")
+                            confidence = data.get("confidence", 0)
+
+                            # Display transcription
+                            marker = "ğŸ¤ FINAL" if is_final else "â¸ï¸  PARTIAL"
+                            print(f"{marker} [{confidence:.2%}] {text}")
+
+                        elif data.get("type") == "topic_update":
+                            topic = data.get("current_topic", "Unknown")
+                            total = data.get("total_topics", 0)
+                            print(f"\nğŸ“Š Topic Update: {topic} (Total: {total})\n")
+
+                        elif data.get("type") == "fact_queued":
+                            sentence = data.get("sentence", "")[:50]
+                            print(f"ğŸ” Fact check queued: {sentence}...")
+
+                        elif data.get("type") == "error":
+                            print(f"âŒ Error: {data.get('message')}")
+
+                except websockets.exceptions.ConnectionClosed:
+                    print("\nâœ“ Connection closed")
+                except Exception as e:
+                    print(f"\nâœ— Error receiving: {e}")
+
+            # Run both tasks concurrently
+            try:
+                await asyncio.gather(
+                    send_audio(),
+                    receive_transcriptions()
+                )
+            except asyncio.CancelledError:
+                print("\nâœ“ Stopping...")
+
+            # Clean up FFmpeg process
+            if process.returncode is None:
+                process.terminate()
+                await process.wait()
+
+    except websockets.exceptions.WebSocketException as e:
+        print(f"âœ— WebSocket error: {e}")
+    except Exception as e:
+        print(f"âœ— Error: {e}")
+
+
+async def test_with_sample_audio():
+    """
+    Alternative: Generate test audio using FFmpeg's built-in test source.
+    Useful if you don't have an audio file or stream.
+    """
+    uri = "ws://localhost:8000/listen"
+
+    print("Generating test audio signal...")
+
+    # Generate 10 seconds of test audio (sine wave)
+    ffmpeg_cmd = [
+        FFMPEG_PATH,
+        '-f', 'lavfi',
+        '-i', 'sine=frequency=1000:duration=10',  # 1kHz sine wave for 10 seconds
+        '-f', 's16le',
+        '-ar', '16000',
+        '-ac', '1',
+        '-loglevel', 'error',
+        '-'
+    ]
+
+    try:
+        async with websockets.connect(uri) as websocket:
+            print(f"âœ“ Connected to {uri}")
+
+            process = await asyncio.create_subprocess_exec(
+                *ffmpeg_cmd,
+                stdout=asyncio.subprocess.PIPE,
+                stderr=asyncio.subprocess.PIPE
+            )
+
+            print("Sending test audio...")
+
+            while True:
+                chunk = await process.stdout.read(2560)
+                if not chunk:
+                    break
+                await websocket.send(chunk)
+
+            print("âœ“ Test audio sent")
+
+            # Wait for any responses
+            await asyncio.sleep(2)
+
+    except Exception as e:
+        print(f"âœ— Error: {e}")
+
+
+async def main():
+    """Main test runner."""
+    print("Real-Time Podcast AI Assistant - Audio Test Client")
+    print("=" * 50)
+    print()
+
+    # Check if FFmpeg exists
+    ffmpeg_path = Path(FFMPEG_PATH)
+    if not ffmpeg_path.exists() and FFMPEG_PATH != "ffmpeg":
+        print(f"âŒ FFmpeg not found at: {FFMPEG_PATH}")
+        print("\n" + "=" * 50)
+        print("FFmpeg Installation Required")
+        print("=" * 50)
+        print("\nOption 1: Download FFmpeg (Recommended)")
+        print("  1. Download: https://www.gyan.dev/ffmpeg/builds/ffmpeg-release-essentials.zip")
+        print("  2. Extract to C:\\ffmpeg")
+        print("  3. Verify ffmpeg.exe is at: C:\\ffmpeg\\bin\\ffmpeg.exe")
+        print("\nOption 2: Use Chocolatey (if installed)")
+        print("  choco install ffmpeg")
+        print("\nOption 3: Update FFMPEG_PATH in test_audio_client.py")
+        print(f"  Current: {FFMPEG_PATH}")
+        print("  Change to your ffmpeg.exe location")
+        print("\n" + "=" * 50)
+        return
+
+    print(f"âœ“ FFmpeg found")
+    print()
+
+    # Stream the configured audio source
+    await stream_audio_to_deepgram()
+
+
+if __name__ == "__main__":
+    try:
+        asyncio.run(main())
+    except KeyboardInterrupt:
+        print("\n\nâœ“ Stopped by user")
diff --git a/test_client.py b/test_client.py
new file mode 100644
index 0000000..927463d
--- /dev/null
+++ b/test_client.py
@@ -0,0 +1,162 @@
+"""
+Simple test client for Real-Time Podcast AI Assistant.
+Simulates text-based transcript for testing without audio.
+"""
+
+import asyncio
+import json
+import websockets
+from datetime import datetime
+
+
+# Sample podcast conversation for testing
+SAMPLE_CONVERSATION = [
+    "Welcome to today's podcast about artificial intelligence.",
+    "We're discussing the latest developments in large language models.",
+    "ChatGPT was released by OpenAI in November 2022.",
+    "It quickly gained over 100 million users within two months.",
+    "That made it the fastest-growing consumer application in history.",
+    "The model is based on transformer architecture.",
+    "Transformers were introduced in the paper 'Attention is All You Need'.",
+    "This was published by Google researchers in 2017.",
+    "The architecture revolutionized natural language processing.",
+    "Now let's talk about climate change impacts.",
+    "Global temperatures have risen by about 1.1 degrees Celsius since pre-industrial times.",
+    "The Paris Agreement aims to limit warming to 1.5 degrees Celsius.",
+    "Arctic sea ice has been declining at a rate of 13% per decade.",
+    "This is one of the most visible signs of climate change.",
+    "Many scientists believe we need immediate action.",
+]
+
+
+async def test_transcript_simulation():
+    """
+    NOTE: This test is currently disabled because the /listen endpoint
+    expects binary audio data (PCM), not JSON text.
+
+    Use test_audio_client.py instead for proper audio streaming tests.
+    """
+    print("âš ï¸  TEXT-BASED TESTING NOT SUPPORTED")
+    print("The /listen endpoint requires binary audio data (linear16 PCM).")
+    print("\nPlease use: python test_audio_client.py")
+    print("This will stream real audio using FFmpeg.\n")
+    return
+
+    # Original code disabled
+    uri = "ws://localhost:8000/listen"
+
+    print(f"Connecting to {uri}...")
+
+    try:
+        async with websockets.connect(uri) as websocket:
+            print("âœ“ Connected to server\n")
+
+            for i, text in enumerate(SAMPLE_CONVERSATION):
+                # Simulate transcript event
+                transcript_event = {
+                    "type": "transcript",
+                    "text": text,
+                    "is_final": True,
+                    "timestamp": datetime.now().isoformat(),
+                }
+
+                print(f"[{i+1}/{len(SAMPLE_CONVERSATION)}] Sending: {text}")
+
+                # Send as text message
+                await websocket.send(json.dumps(transcript_event))
+
+                # Receive response
+                try:
+                    response = await asyncio.wait_for(
+                        websocket.recv(), timeout=2.0
+                    )
+                    response_data = json.loads(response)
+
+                    # Print server response
+                    if response_data.get("type") == "topic_update":
+                        print(
+                            f"  â†’ Topic: {response_data.get('current_topic')} "
+                            f"(Total: {response_data.get('total_topics')})"
+                        )
+
+                    elif response_data.get("type") == "fact_queued":
+                        print(
+                            f"  â†’ Fact check queued (Queue: {response_data.get('queue_size')})"
+                        )
+
+                except asyncio.TimeoutError:
+                    pass
+
+                # Wait between messages
+                await asyncio.sleep(1)
+
+            print("\nâœ“ All messages sent successfully")
+            print("\nWaiting 5 seconds for final fact checks...")
+            await asyncio.sleep(5)
+
+    except websockets.exceptions.WebSocketException as e:
+        print(f"âœ— WebSocket error: {e}")
+    except Exception as e:
+        print(f"âœ— Error: {e}")
+
+
+async def test_api_endpoints():
+    """Test the REST API endpoints."""
+    import aiohttp
+
+    base_url = "http://localhost:8000"
+
+    print("Testing API Endpoints")
+    print("=" * 50)
+
+    async with aiohttp.ClientSession() as session:
+        # Test health check
+        async with session.get(f"{base_url}/") as resp:
+            data = await resp.json()
+            print(f"âœ“ Health Check: {data.get('status')}")
+
+        # Test stats
+        async with session.get(f"{base_url}/stats") as resp:
+            data = await resp.json()
+            print(f"âœ“ Stats: {data}")
+
+        # Test topics
+        async with session.get(f"{base_url}/topics") as resp:
+            data = await resp.json()
+            print(f"âœ“ Topics: {data.get('total_topics')} topics found")
+
+        # Test facts
+        async with session.get(f"{base_url}/facts") as resp:
+            data = await resp.json()
+            print(f"âœ“ Facts: {data.get('total')} fact checks performed")
+
+
+async def main():
+    """Main test runner."""
+    print("Real-Time Podcast AI Assistant - Test Client")
+    print("=" * 50)
+    print()
+
+    # First test API endpoints
+    print("1. Testing API Endpoints...")
+    try:
+        await test_api_endpoints()
+    except Exception as e:
+        print(f"âœ— API test failed: {e}")
+        print("Make sure the server is running (python main.py)")
+        return
+
+    print()
+    print("2. Testing WebSocket Transcript Simulation...")
+    await test_transcript_simulation()
+
+    print()
+    print("=" * 50)
+    print("Testing complete!")
+    print()
+    print("Check the server logs for detailed processing information.")
+    print("Visit http://localhost:8000/facts to see fact-checking results.")
+
+
+if __name__ == "__main__":
+    asyncio.run(main())
diff --git a/backend/tests/test_wav_stream.py b/test_wav_stream.py
similarity index 97%
rename from backend/tests/test_wav_stream.py
rename to test_wav_stream.py
index 811d7b5..5682edf 100644
--- a/backend/tests/test_wav_stream.py
+++ b/test_wav_stream.py
@@ -12,13 +12,8 @@ from pathlib import Path
 from datetime import datetime
 
 
-# WAV file path - using relative path from this file's location
-# Get the directory where this script is located
-SCRIPT_DIR = Path(__file__).parent
-TEST_DATA_DIR = SCRIPT_DIR / "test_data"
-
-# WAV_FILE_PATH = TEST_DATA_DIR / "audio.wav"
-WAV_FILE_PATH = TEST_DATA_DIR / "LexNuclear.wav"
+# WAV file path - UPDATE THIS to your WAV file location
+WAV_FILE_PATH = "audio.wav"
 
 # Streaming configuration
 CHUNK_DURATION_MS = 100  # Send chunks every 100ms (simulates real-time)
@@ -131,7 +126,11 @@ async def stream_wav_file():
                             elif data.get("type") == "topic_update":
                                 topic = data.get("current_topic", "Unknown")
                                 total = data.get("total_topics", 0)
-                                print(f"\nğŸ“Š TOPIC UPDATE: {topic} (Total topics: {total})\n")
+                                image_url = data.get("image_url")
+                                print(f"\nğŸ“Š TOPIC UPDATE: {topic} (Total topics: {total})")
+                                if image_url:
+                                    print(f"   ğŸ–¼ï¸  Image: {image_url}")
+                                print()
 
                             elif data.get("type") == "fact_queued":
                                 sentence = data.get("sentence", "")[:60]
diff --git a/backend/app/engines/topic_engine.py b/topic_engine.py
similarity index 52%
rename from backend/app/engines/topic_engine.py
rename to topic_engine.py
index 9103726..c370a49 100644
--- a/backend/app/engines/topic_engine.py
+++ b/topic_engine.py
@@ -3,15 +3,20 @@ Topic Engine for Real-Time Podcast AI Assistant.
 Handles semantic drift detection and topic tree updates.
 """
 
+import asyncio
 import json
 import logging
 from datetime import datetime
 from typing import Dict, List, Optional, Tuple
 import numpy as np
 from together import Together
+try:
+    from ddgs import DDGS
+except ImportError:
+    from duckduckgo_search import DDGS  # Fallback for older package name
 
-from backend.app.core.config import settings, TOPIC_EXTRACTION_PROMPT, TOPIC_CONFIG
-from backend.app.core.state_manager import state
+from config import settings, TOPIC_EXTRACTION_PROMPT, TOPIC_CONFIG
+from state_manager import state
 
 logger = logging.getLogger(__name__)
 
@@ -25,6 +30,61 @@ class TopicEngine:
     def __init__(self):
         self.client = Together(api_key=settings.together_api_key)
         self.embedding_cache: Dict[str, np.ndarray] = {}
+        self.search_client = DDGS()
+
+    async def search_topic_image(self, topic: str, keywords: List[str]) -> Optional[str]:
+        """
+        Search for a relevant image for the topic.
+        
+        Args:
+            topic: The topic name
+            keywords: Topic keywords to enhance search
+            
+        Returns:
+            URL of the most relevant image, or None if search fails
+        """
+        try:
+            # Create search query from topic and keywords
+            search_terms = [topic] + keywords[:3]  # Use top 3 keywords
+            query = " ".join(search_terms)
+            
+            logger.info(f"Searching images for: {query}")
+            print(f"ğŸ–¼ï¸  Searching image for: {query}")
+            
+            # Run image search in thread pool
+            loop = asyncio.get_event_loop()
+            results = await loop.run_in_executor(
+                None,
+                lambda: list(
+                    self.search_client.images(
+                        query,  # Changed from keywords= to positional argument
+                        region="wt-wt",
+                        safesearch="moderate",
+                        max_results=3,
+                    )
+                ),
+            )
+            
+            print(f"ğŸ” Image search returned {len(results) if results else 0} results")
+            
+            if results and len(results) > 0:
+                # Return the first (most relevant) image URL
+                image_url = results[0].get("image")
+                print(f"   First result: {results[0]}")
+                if image_url:
+                    print(f"âœ… Found image: {image_url[:80]}...")
+                    return image_url
+                else:
+                    print(f"âŒ No 'image' key in result")
+            
+            print(f"âš ï¸  No images found for: {query}")
+            logger.warning(f"No images found for: {query}")
+            return None
+            
+        except Exception as e:
+            print(f"âŒ Image search error: {e}")
+            logger.error(f"Image search failed: {e}")
+            return None
 
     async def extract_topic(self, text: str) -> Optional[Tuple[str, List[str]]]:
         """
@@ -147,34 +207,56 @@ class TopicEngine:
         # Ensure in [0, 1] range
         return float(max(0.0, min(1.0, similarity)))
 
-    def detect_topic_shift(self, new_topic: str) -> bool:
+    def find_existing_topic(self, new_topic: str) -> Optional[str]:
         """
-        Detect if there's a significant topic shift.
-
+        Check if this topic already exists in the tree.
+        
+        If a highly similar topic exists, return its ID instead of creating a duplicate.
+        
         Args:
-            new_topic: Newly extracted topic
-
+            new_topic: The new topic text to compare
+            
         Returns:
-            True if this represents a new topic (shift detected)
+            topic_id if match found, None otherwise
         """
-        # If no current topic, this is definitely new
-        if state.current_topic_id is None:
-            return True
-
-        # Get current topic text
-        current_node = state.topic_tree.nodes[state.current_topic_id]["data"]
-        current_topic_text = current_node.topic
+        if len(state.topic_tree.nodes) == 0:
+            return None
+        
+        # Check all existing topics
+        for topic_id in state.topic_tree.nodes:
+            node_data = state.topic_tree.nodes[topic_id]["data"]
+            topic_text = node_data.topic
+            
+            similarity = self.compute_similarity(topic_text, new_topic)
+            
+            # If very similar, consider it the same topic
+            if similarity >= TOPIC_CONFIG["similarity_threshold"]:
+                logger.info(f"Found existing topic: '{topic_text}' (similarity: {similarity:.2f})")
+                return topic_id
+        
+        return None
 
-        # Compute similarity
-        similarity = self.compute_similarity(current_topic_text, new_topic)
+    def detect_topic_shift(self, new_topic: str) -> Tuple[bool, Optional[str]]:
+        """
+        Check if this is a new topic or returning to an existing one.
 
-        logger.info(
-            f"Topic similarity: {similarity:.2f} "
-            f"(current: '{current_topic_text}', new: '{new_topic}')"
-        )
+        Args:
+            new_topic: Newly extracted topic
 
-        # If similarity is below threshold, it's a new topic
-        return similarity < TOPIC_CONFIG["similarity_threshold"]
+        Returns:
+            Tuple of (is_new_topic, existing_topic_id)
+            - is_new_topic: True if we need to create a new topic node
+            - existing_topic_id: ID of existing topic if found, None otherwise
+        """
+        # Check if this topic already exists
+        existing_id = self.find_existing_topic(new_topic)
+        
+        if existing_id:
+            # Found existing topic - reuse it
+            return (False, existing_id)
+        else:
+            # New topic - will be created
+            return (True, None)
 
     async def update_topic_tree(self, text: str) -> Optional[str]:
         """
@@ -197,23 +279,51 @@ class TopicEngine:
 
             topic, keywords = result
 
-            # Check if this is a new topic (semantic drift)
-            if self.detect_topic_shift(topic):
-                # Create new topic node
+            # Check if this topic already exists
+            is_new_topic, existing_topic_id = self.detect_topic_shift(topic)
+            
+            if is_new_topic:
+                # Create new topic node (without image for now)
                 topic_id = state.add_topic_node(
-                    topic=topic, keywords=keywords, timestamp=datetime.now()
+                    topic=topic, 
+                    keywords=keywords, 
+                    timestamp=datetime.now()
                 )
-                logger.info(f"New topic detected: {topic} (id={topic_id})")
+                logger.info(f"New topic: {topic} (id={topic_id})")
+                
+                # Search for image asynchronously (don't block)
+                asyncio.create_task(self._search_and_record_image(topic_id, topic, keywords))
+                
                 return topic_id
             else:
-                # Update existing topic
-                state.update_current_topic()
-                logger.info(f"Continuing current topic: {topic}")
-                return state.current_topic_id
+                # Returning to existing topic - switch to it
+                state.switch_to_topic(existing_topic_id)
+                logger.info(f"Returning to existing topic: {topic} (id={existing_topic_id})")
+                return existing_topic_id
 
         except Exception as e:
             logger.error(f"Failed to update topic tree: {e}")
             return None
+    
+    async def _search_and_record_image(self, topic_id: str, topic: str, keywords: List[str]) -> None:
+        """
+        Search for topic image and record it (called as background task).
+        
+        Args:
+            topic_id: Topic ID
+            topic: Topic name
+            keywords: Topic keywords
+        """
+        try:
+            print(f"ğŸ”„ Starting image search task for: {topic}")
+            image_url = await self.search_topic_image(topic, keywords)
+            print(f"ğŸ“ Recording image for {topic_id}: {image_url}")
+            state.add_topic_image(topic_id, topic, image_url)
+            print(f"âœ… Image recorded successfully")
+        except Exception as e:
+            print(f"âŒ Failed to search/record image: {e}")
+            logger.error(f"Failed to search/record image: {e}")
+            state.add_topic_image(topic_id, topic, None)
 
     def get_topic_summary(self) -> Dict:
         """
diff --git a/verify_setup.py b/verify_setup.py
new file mode 100644
index 0000000..2b83832
--- /dev/null
+++ b/verify_setup.py
@@ -0,0 +1,200 @@
+"""
+Setup verification script for Real-Time Podcast AI Assistant.
+Checks if all dependencies are installed and configured correctly.
+"""
+
+import sys
+
+
+def check_imports():
+    """Check if all required packages can be imported."""
+    print("Checking package imports...")
+    print("=" * 50)
+
+    packages = {
+        "fastapi": "FastAPI",
+        "uvicorn": "Uvicorn",
+        "deepgram": "Deepgram SDK",
+        "together": "Together AI",
+        "duckduckgo_search": "DuckDuckGo Search",
+        "networkx": "NetworkX",
+        "aiohttp": "aiohttp",
+        "pydantic": "Pydantic",
+        "pydantic_settings": "Pydantic Settings",
+        "dotenv": "python-dotenv",
+    }
+
+    failed = []
+    for module, name in packages.items():
+        try:
+            __import__(module)
+            print(f"âœ“ {name}")
+        except ImportError as e:
+            print(f"âœ— {name}: {e}")
+            failed.append(name)
+
+    print()
+    return failed
+
+
+def check_env_file():
+    """Check if .env file exists and has required keys."""
+    print("Checking environment configuration...")
+    print("=" * 50)
+
+    import os
+    from pathlib import Path
+
+    env_path = Path(".env")
+
+    if not env_path.exists():
+        print("âœ— .env file not found")
+        print("  Run: cp .env.example .env")
+        print("  Then edit .env and add your API keys")
+        return False
+
+    print("âœ“ .env file found")
+
+    # Check if keys are set
+    try:
+        from dotenv import load_dotenv
+
+        load_dotenv()
+
+        required_keys = ["DEEPGRAM_API_KEY", "TOGETHER_API_KEY"]
+        missing = []
+
+        for key in required_keys:
+            value = os.getenv(key)
+            if not value or value.startswith("your_"):
+                print(f"âœ— {key} not configured")
+                missing.append(key)
+            else:
+                print(f"âœ“ {key} configured")
+
+        print()
+        return len(missing) == 0
+
+    except Exception as e:
+        print(f"âœ— Error loading .env: {e}")
+        return False
+
+
+def check_modules():
+    """Check if our custom modules can be imported."""
+    print("Checking custom modules...")
+    print("=" * 50)
+
+    modules = [
+        "config",
+        "state_manager",
+        "topic_engine",
+        "fact_engine",
+        "main",
+    ]
+
+    failed = []
+    for module in modules:
+        try:
+            __import__(module)
+            print(f"âœ“ {module}.py")
+        except Exception as e:
+            print(f"âœ— {module}.py: {e}")
+            failed.append(module)
+
+    print()
+    return failed
+
+
+def check_deepgram_version():
+    """Check Deepgram SDK version."""
+    print("Checking Deepgram SDK version...")
+    print("=" * 50)
+
+    try:
+        import deepgram
+
+        # Try to get version
+        if hasattr(deepgram, "__version__"):
+            version = deepgram.__version__
+            print(f"âœ“ Deepgram SDK version: {version}")
+
+            # Check if it's 5.x
+            major_version = int(version.split(".")[0])
+            if major_version >= 5:
+                print("âœ“ Version 5.x or higher (compatible)")
+            else:
+                print(f"âœ— Version {version} detected, need 5.x or higher")
+                print("  Run: pip install --upgrade deepgram-sdk")
+                return False
+        else:
+            print("âœ“ Deepgram SDK installed (version check unavailable)")
+
+        # Try to import key classes
+        from deepgram import DeepgramClient, LiveOptions, LiveTranscriptionEvents
+
+        print("âœ“ All required Deepgram classes available")
+        print()
+        return True
+
+    except ImportError as e:
+        print(f"âœ— Deepgram SDK import failed: {e}")
+        print()
+        return False
+
+
+def main():
+    """Run all verification checks."""
+    print("\n" + "=" * 50)
+    print("Real-Time Podcast AI Assistant - Setup Verification")
+    print("=" * 50)
+    print()
+
+    all_passed = True
+
+    # Check imports
+    failed_imports = check_imports()
+    if failed_imports:
+        all_passed = False
+        print(f"âš  Failed to import: {', '.join(failed_imports)}")
+        print("Run: pip install -r requirements.txt")
+        print()
+
+    # Check Deepgram version
+    if not check_deepgram_version():
+        all_passed = False
+
+    # Check environment
+    if not check_env_file():
+        all_passed = False
+
+    # Check custom modules (only if imports succeeded)
+    if not failed_imports:
+        failed_modules = check_modules()
+        if failed_modules:
+            all_passed = False
+    else:
+        print("âš  Skipping custom module checks (dependencies missing)")
+        print()
+
+    # Final summary
+    print("=" * 50)
+    if all_passed:
+        print("âœ“ All checks passed!")
+        print()
+        print("You're ready to start the server:")
+        print("  python main.py")
+        print()
+        print("Or run the test client:")
+        print("  python test_client.py")
+    else:
+        print("âœ— Some checks failed")
+        print()
+        print("Please fix the issues above before running the server.")
+        sys.exit(1)
+
+    print("=" * 50)
+
+
+if __name__ == "__main__":
+    main()
